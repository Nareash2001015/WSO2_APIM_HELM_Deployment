* 
* ==> Audit <==
* |--------------|----------------|----------|---------|---------|-----------------------|-----------------------|
|   Command    |      Args      | Profile  |  User   | Version |      Start Time       |       End Time        |
|--------------|----------------|----------|---------|---------|-----------------------|-----------------------|
| start        |                | minikube | nareash | v1.32.0 | 12 Dec 23 14:45 +0530 | 12 Dec 23 14:54 +0530 |
| update-check |                | minikube | nareash | v1.32.0 | 13 Dec 23 00:06 +0530 | 13 Dec 23 00:06 +0530 |
| ip           |                | minikube | nareash | v1.32.0 | 13 Dec 23 08:38 +0530 | 13 Dec 23 08:38 +0530 |
| ip           |                | minikube | nareash | v1.32.0 | 13 Dec 23 08:39 +0530 | 13 Dec 23 08:39 +0530 |
| service      | web-app.yaml   | minikube | nareash | v1.32.0 | 13 Dec 23 08:40 +0530 |                       |
| service      | webapp-service | minikube | nareash | v1.32.0 | 13 Dec 23 08:40 +0530 | 13 Dec 23 09:52 +0530 |
| stop         |                | minikube | nareash | v1.32.0 | 13 Dec 23 09:53 +0530 | 13 Dec 23 09:53 +0530 |
| update-check |                | minikube | nareash | v1.32.0 | 19 Dec 23 12:16 +0530 | 19 Dec 23 12:16 +0530 |
| update-check |                | minikube | nareash | v1.32.0 | 21 Dec 23 23:01 +0530 | 21 Dec 23 23:01 +0530 |
| update-check |                | minikube | nareash | v1.32.0 | 03 Jan 24 09:39 +0530 | 03 Jan 24 09:39 +0530 |
| update-check |                | minikube | nareash | v1.32.0 | 12 Jan 24 13:18 +0530 | 12 Jan 24 13:18 +0530 |
| update-check |                | minikube | nareash | v1.32.0 | 16 Jan 24 11:26 +0530 | 16 Jan 24 11:26 +0530 |
| update-check |                | minikube | nareash | v1.32.0 | 16 Jan 24 14:56 +0530 | 16 Jan 24 14:56 +0530 |
| update-check |                | minikube | nareash | v1.32.0 | 17 Jan 24 08:55 +0530 | 17 Jan 24 08:55 +0530 |
| update-check |                | minikube | nareash | v1.32.0 | 18 Jan 24 21:09 +0530 | 18 Jan 24 21:09 +0530 |
| update-check |                | minikube | nareash | v1.32.0 | 19 Jan 24 10:46 +0530 | 19 Jan 24 10:46 +0530 |
| start        |                | minikube | nareash | v1.32.0 | 19 Jan 24 12:04 +0530 | 19 Jan 24 12:05 +0530 |
| start        |                | minikube | nareash | v1.32.0 | 19 Jan 24 12:07 +0530 | 19 Jan 24 12:09 +0530 |
| delete       |                | minikube | nareash | v1.32.0 | 19 Jan 24 13:18 +0530 | 19 Jan 24 13:18 +0530 |
| start        |                | minikube | nareash | v1.32.0 | 19 Jan 24 13:21 +0530 | 19 Jan 24 13:23 +0530 |
| start        |                | minikube | nareash | v1.32.0 | 19 Jan 24 13:39 +0530 | 19 Jan 24 13:40 +0530 |
| ip           |                | minikube | nareash | v1.32.0 | 19 Jan 24 13:53 +0530 | 19 Jan 24 13:53 +0530 |
| ip           |                | minikube | nareash | v1.32.0 | 19 Jan 24 13:55 +0530 | 19 Jan 24 13:55 +0530 |
| ip           |                | minikube | nareash | v1.32.0 | 19 Jan 24 13:59 +0530 | 19 Jan 24 13:59 +0530 |
| ip           |                | minikube | nareash | v1.32.0 | 19 Jan 24 14:44 +0530 | 19 Jan 24 14:44 +0530 |
| service      | nginx-service  | minikube | nareash | v1.32.0 | 19 Jan 24 14:55 +0530 | 19 Jan 24 14:55 +0530 |
| service      | nginx-service  | minikube | nareash | v1.32.0 | 19 Jan 24 15:00 +0530 | 19 Jan 24 15:00 +0530 |
| ip           |                | minikube | nareash | v1.32.0 | 19 Jan 24 16:03 +0530 | 19 Jan 24 16:03 +0530 |
| service      | nginx-service  | minikube | nareash | v1.32.0 | 19 Jan 24 16:04 +0530 | 19 Jan 24 16:07 +0530 |
| service      | list           | minikube | nareash | v1.32.0 | 19 Jan 24 16:07 +0530 | 19 Jan 24 16:07 +0530 |
| update-check |                | minikube | nareash | v1.32.0 | 19 Jan 24 16:38 +0530 | 19 Jan 24 16:38 +0530 |
| ip           |                | minikube | nareash | v1.32.0 | 19 Jan 24 20:38 +0530 | 19 Jan 24 20:38 +0530 |
| service      | apim-service   | minikube | nareash | v1.32.0 | 19 Jan 24 20:40 +0530 | 19 Jan 24 20:54 +0530 |
| service      | list           | minikube | nareash | v1.32.0 | 19 Jan 24 20:58 +0530 | 19 Jan 24 20:58 +0530 |
| service      | list           | minikube | nareash | v1.32.0 | 19 Jan 24 20:59 +0530 | 19 Jan 24 20:59 +0530 |
| service      | list           | minikube | nareash | v1.32.0 | 19 Jan 24 21:03 +0530 | 19 Jan 24 21:03 +0530 |
| service      | nginx-service  | minikube | nareash | v1.32.0 | 19 Jan 24 21:08 +0530 | 19 Jan 24 21:08 +0530 |
| service      | apim-service   | minikube | nareash | v1.32.0 | 19 Jan 24 21:09 +0530 |                       |
| service      | list           | minikube | nareash | v1.32.0 | 19 Jan 24 21:10 +0530 | 19 Jan 24 21:10 +0530 |
| service      | apim-service   | minikube | nareash | v1.32.0 | 19 Jan 24 21:10 +0530 |                       |
| service      | list           | minikube | nareash | v1.32.0 | 19 Jan 24 21:12 +0530 | 19 Jan 24 21:12 +0530 |
| service      | apim-service   | minikube | nareash | v1.32.0 | 19 Jan 24 21:14 +0530 |                       |
| service      | list           | minikube | nareash | v1.32.0 | 19 Jan 24 21:39 +0530 | 19 Jan 24 21:39 +0530 |
| service      | apim-service   | minikube | nareash | v1.32.0 | 19 Jan 24 21:39 +0530 |                       |
| service      | list           | minikube | nareash | v1.32.0 | 19 Jan 24 21:44 +0530 | 19 Jan 24 21:44 +0530 |
| service      | apim-service   | minikube | nareash | v1.32.0 | 19 Jan 24 21:45 +0530 |                       |
|--------------|----------------|----------|---------|---------|-----------------------|-----------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/01/19 13:39:52
Running on machine: nareash
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0119 13:39:52.240161    5771 out.go:296] Setting OutFile to fd 1 ...
I0119 13:39:52.240725    5771 out.go:348] isatty.IsTerminal(1) = true
I0119 13:39:52.240738    5771 out.go:309] Setting ErrFile to fd 2...
I0119 13:39:52.240754    5771 out.go:348] isatty.IsTerminal(2) = true
I0119 13:39:52.241491    5771 root.go:338] Updating PATH: /home/nareash/.minikube/bin
I0119 13:39:52.243088    5771 out.go:303] Setting JSON to false
I0119 13:39:52.252161    5771 start.go:128] hostinfo: {"hostname":"nareash","uptime":110,"bootTime":1705651682,"procs":356,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.5.0-14-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"4588d8bf-b5b7-4bbf-8607-59a363acab17"}
I0119 13:39:52.252503    5771 start.go:138] virtualization: kvm host
I0119 13:39:52.255945    5771 out.go:177] üòÑ  minikube v1.32.0 on Ubuntu 22.04
I0119 13:39:52.258842    5771 notify.go:220] Checking for updates...
I0119 13:39:52.260803    5771 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0119 13:39:52.261699    5771 driver.go:378] Setting default libvirt URI to qemu:///system
I0119 13:39:52.564058    5771 docker.go:122] docker version: linux-24.0.7:Docker Desktop 4.26.0 (130397)
I0119 13:39:52.564343    5771 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0119 13:39:53.273502    5771 info.go:266] docker info: {ID:1bc14d3d-af63-438e-99af-ebfa9fbe7548 Containers:12 ContainersRunning:8 ContainersPaused:0 ContainersStopped:4 Images:15 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:75 OomKillDisable:false NGoroutines:83 SystemTime:2024-01-19 08:09:53.214626381 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:10 KernelVersion:6.5.11-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3880570880 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f Expected:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.0-desktop.2] map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.3-desktop.2] map[Name:dev Path:/usr/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/usr/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:/usr/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:0.1] map[Name:init Path:/usr/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.10] map[Name:sbom Path:/usr/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/usr/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.2.0]] Warnings:<nil>}}
I0119 13:39:53.273923    5771 docker.go:295] overlay module found
I0119 13:39:53.276082    5771 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0119 13:39:53.277240    5771 start.go:298] selected driver: docker
I0119 13:39:53.277263    5771 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3652 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/nareash:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0119 13:39:53.277565    5771 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0119 13:39:53.277816    5771 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0119 13:39:53.675046    5771 info.go:266] docker info: {ID:1bc14d3d-af63-438e-99af-ebfa9fbe7548 Containers:12 ContainersRunning:8 ContainersPaused:0 ContainersStopped:4 Images:15 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:75 OomKillDisable:false NGoroutines:83 SystemTime:2024-01-19 08:09:53.624140918 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:10 KernelVersion:6.5.11-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3880570880 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f Expected:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.0-desktop.2] map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.3-desktop.2] map[Name:dev Path:/usr/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/usr/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:/usr/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:0.1] map[Name:init Path:/usr/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.10] map[Name:sbom Path:/usr/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/usr/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.2.0]] Warnings:<nil>}}
I0119 13:39:53.679451    5771 cni.go:84] Creating CNI manager for ""
I0119 13:39:53.679525    5771 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0119 13:39:53.679568    5771 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3652 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/nareash:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0119 13:39:53.682972    5771 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0119 13:39:53.684866    5771 cache.go:121] Beginning downloading kic base image for docker with docker
I0119 13:39:53.686652    5771 out.go:177] üöú  Pulling base image ...
I0119 13:39:53.689619    5771 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0119 13:39:53.689762    5771 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0119 13:39:53.689970    5771 preload.go:148] Found local preload: /home/nareash/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0119 13:39:53.690005    5771 cache.go:56] Caching tarball of preloaded images
I0119 13:39:53.690483    5771 preload.go:174] Found /home/nareash/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0119 13:39:53.690553    5771 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0119 13:39:53.690922    5771 profile.go:148] Saving config to /home/nareash/.minikube/profiles/minikube/config.json ...
I0119 13:39:53.933733    5771 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0119 13:39:53.933771    5771 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0119 13:39:53.933818    5771 cache.go:194] Successfully downloaded all kic artifacts
I0119 13:39:53.933918    5771 start.go:365] acquiring machines lock for minikube: {Name:mkef7cced7e8a14b3efda5f8a62c6b97095924b0 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0119 13:39:53.934228    5771 start.go:369] acquired machines lock for "minikube" in 252.153¬µs
I0119 13:39:53.934274    5771 start.go:96] Skipping create...Using existing machine configuration
I0119 13:39:53.934288    5771 fix.go:54] fixHost starting: 
I0119 13:39:53.935196    5771 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 13:39:54.154613    5771 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0119 13:39:54.154652    5771 fix.go:128] unexpected machine state, will restart: <nil>
I0119 13:39:54.158245    5771 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0119 13:39:54.159427    5771 cli_runner.go:164] Run: docker start minikube
I0119 13:39:55.795206    5771 cli_runner.go:217] Completed: docker start minikube: (1.635708143s)
I0119 13:39:55.795373    5771 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 13:39:56.140354    5771 kic.go:430] container "minikube" state is running.
I0119 13:39:56.141991    5771 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0119 13:39:56.521441    5771 profile.go:148] Saving config to /home/nareash/.minikube/profiles/minikube/config.json ...
I0119 13:39:56.522237    5771 machine.go:88] provisioning docker machine ...
I0119 13:39:56.522362    5771 ubuntu.go:169] provisioning hostname "minikube"
I0119 13:39:56.522544    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 13:39:56.845657    5771 main.go:141] libmachine: Using SSH client type: native
I0119 13:39:56.848097    5771 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43813 <nil> <nil>}
I0119 13:39:56.848166    5771 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0119 13:39:56.861273    5771 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0119 13:40:00.448825    5771 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0119 13:40:00.448998    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 13:40:00.722109    5771 main.go:141] libmachine: Using SSH client type: native
I0119 13:40:00.723959    5771 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43813 <nil> <nil>}
I0119 13:40:00.724035    5771 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0119 13:40:01.066073    5771 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0119 13:40:01.066153    5771 ubuntu.go:175] set auth options {CertDir:/home/nareash/.minikube CaCertPath:/home/nareash/.minikube/certs/ca.pem CaPrivateKeyPath:/home/nareash/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/nareash/.minikube/machines/server.pem ServerKeyPath:/home/nareash/.minikube/machines/server-key.pem ClientKeyPath:/home/nareash/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/nareash/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/nareash/.minikube}
I0119 13:40:01.066260    5771 ubuntu.go:177] setting up certificates
I0119 13:40:01.066283    5771 provision.go:83] configureAuth start
I0119 13:40:01.066451    5771 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0119 13:40:01.255519    5771 provision.go:138] copyHostCerts
I0119 13:40:01.255901    5771 exec_runner.go:144] found /home/nareash/.minikube/ca.pem, removing ...
I0119 13:40:01.255939    5771 exec_runner.go:203] rm: /home/nareash/.minikube/ca.pem
I0119 13:40:01.256067    5771 exec_runner.go:151] cp: /home/nareash/.minikube/certs/ca.pem --> /home/nareash/.minikube/ca.pem (1078 bytes)
I0119 13:40:01.256542    5771 exec_runner.go:144] found /home/nareash/.minikube/cert.pem, removing ...
I0119 13:40:01.256557    5771 exec_runner.go:203] rm: /home/nareash/.minikube/cert.pem
I0119 13:40:01.256628    5771 exec_runner.go:151] cp: /home/nareash/.minikube/certs/cert.pem --> /home/nareash/.minikube/cert.pem (1123 bytes)
I0119 13:40:01.256997    5771 exec_runner.go:144] found /home/nareash/.minikube/key.pem, removing ...
I0119 13:40:01.257009    5771 exec_runner.go:203] rm: /home/nareash/.minikube/key.pem
I0119 13:40:01.257075    5771 exec_runner.go:151] cp: /home/nareash/.minikube/certs/key.pem --> /home/nareash/.minikube/key.pem (1675 bytes)
I0119 13:40:01.257462    5771 provision.go:112] generating server cert: /home/nareash/.minikube/machines/server.pem ca-key=/home/nareash/.minikube/certs/ca.pem private-key=/home/nareash/.minikube/certs/ca-key.pem org=nareash.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0119 13:40:01.775285    5771 provision.go:172] copyRemoteCerts
I0119 13:40:01.775513    5771 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0119 13:40:01.775640    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 13:40:02.002193    5771 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:43813 SSHKeyPath:/home/nareash/.minikube/machines/minikube/id_rsa Username:docker}
I0119 13:40:02.218153    5771 ssh_runner.go:362] scp /home/nareash/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0119 13:40:02.321135    5771 ssh_runner.go:362] scp /home/nareash/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I0119 13:40:02.450189    5771 ssh_runner.go:362] scp /home/nareash/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0119 13:40:02.559328    5771 provision.go:86] duration metric: configureAuth took 1.493016899s
I0119 13:40:02.559365    5771 ubuntu.go:193] setting minikube options for container-runtime
I0119 13:40:02.559888    5771 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0119 13:40:02.560069    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 13:40:02.725875    5771 main.go:141] libmachine: Using SSH client type: native
I0119 13:40:02.726827    5771 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43813 <nil> <nil>}
I0119 13:40:02.726848    5771 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0119 13:40:03.057853    5771 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0119 13:40:03.057957    5771 ubuntu.go:71] root file system type: overlay
I0119 13:40:03.058336    5771 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0119 13:40:03.058512    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 13:40:03.210897    5771 main.go:141] libmachine: Using SSH client type: native
I0119 13:40:03.212503    5771 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43813 <nil> <nil>}
I0119 13:40:03.212829    5771 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0119 13:40:03.506586    5771 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0119 13:40:03.506759    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 13:40:03.654555    5771 main.go:141] libmachine: Using SSH client type: native
I0119 13:40:03.655590    5771 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43813 <nil> <nil>}
I0119 13:40:03.655630    5771 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0119 13:40:03.914625    5771 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0119 13:40:03.914697    5771 machine.go:91] provisioned docker machine in 7.392408063s
I0119 13:40:03.914723    5771 start.go:300] post-start starting for "minikube" (driver="docker")
I0119 13:40:03.914756    5771 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0119 13:40:03.914953    5771 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0119 13:40:03.915263    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 13:40:04.103833    5771 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:43813 SSHKeyPath:/home/nareash/.minikube/machines/minikube/id_rsa Username:docker}
I0119 13:40:04.271829    5771 ssh_runner.go:195] Run: cat /etc/os-release
I0119 13:40:04.293276    5771 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0119 13:40:04.293385    5771 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0119 13:40:04.293439    5771 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0119 13:40:04.293460    5771 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0119 13:40:04.293488    5771 filesync.go:126] Scanning /home/nareash/.minikube/addons for local assets ...
I0119 13:40:04.295923    5771 filesync.go:126] Scanning /home/nareash/.minikube/files for local assets ...
I0119 13:40:04.296255    5771 start.go:303] post-start completed in 381.506183ms
I0119 13:40:04.296448    5771 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0119 13:40:04.296558    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 13:40:04.531474    5771 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:43813 SSHKeyPath:/home/nareash/.minikube/machines/minikube/id_rsa Username:docker}
I0119 13:40:04.677997    5771 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0119 13:40:04.694688    5771 fix.go:56] fixHost completed within 10.760390346s
I0119 13:40:04.694719    5771 start.go:83] releasing machines lock for "minikube", held for 10.760466736s
I0119 13:40:04.694888    5771 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0119 13:40:04.880801    5771 ssh_runner.go:195] Run: cat /version.json
I0119 13:40:04.880943    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 13:40:04.880976    5771 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0119 13:40:04.881129    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 13:40:05.039540    5771 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:43813 SSHKeyPath:/home/nareash/.minikube/machines/minikube/id_rsa Username:docker}
I0119 13:40:05.079913    5771 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:43813 SSHKeyPath:/home/nareash/.minikube/machines/minikube/id_rsa Username:docker}
I0119 13:40:06.046145    5771 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.16512028s)
I0119 13:40:06.046284    5771 ssh_runner.go:235] Completed: cat /version.json: (1.165442114s)
I0119 13:40:06.046566    5771 ssh_runner.go:195] Run: systemctl --version
I0119 13:40:06.071332    5771 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0119 13:40:06.087698    5771 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0119 13:40:06.157625    5771 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0119 13:40:06.157808    5771 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0119 13:40:06.192448    5771 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0119 13:40:06.192487    5771 start.go:472] detecting cgroup driver to use...
I0119 13:40:06.192560    5771 detect.go:199] detected "systemd" cgroup driver on host os
I0119 13:40:06.192940    5771 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0119 13:40:06.257454    5771 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0119 13:40:06.325325    5771 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0119 13:40:06.389013    5771 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I0119 13:40:06.389231    5771 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0119 13:40:06.445583    5771 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0119 13:40:06.487255    5771 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0119 13:40:06.527477    5771 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0119 13:40:06.568857    5771 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0119 13:40:06.606066    5771 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0119 13:40:06.639836    5771 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0119 13:40:06.677087    5771 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0119 13:40:06.708829    5771 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 13:40:06.969481    5771 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0119 13:40:07.223633    5771 start.go:472] detecting cgroup driver to use...
I0119 13:40:07.223723    5771 detect.go:199] detected "systemd" cgroup driver on host os
I0119 13:40:07.223867    5771 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0119 13:40:07.283263    5771 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0119 13:40:07.283433    5771 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0119 13:40:07.344900    5771 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0119 13:40:07.424918    5771 ssh_runner.go:195] Run: which cri-dockerd
I0119 13:40:07.443943    5771 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0119 13:40:07.486371    5771 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0119 13:40:07.590917    5771 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0119 13:40:07.879398    5771 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0119 13:40:08.117605    5771 docker.go:560] configuring docker to use "systemd" as cgroup driver...
I0119 13:40:08.117950    5771 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0119 13:40:08.192258    5771 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 13:40:08.416228    5771 ssh_runner.go:195] Run: sudo systemctl restart docker
I0119 13:40:09.257025    5771 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0119 13:40:09.533974    5771 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0119 13:40:09.759726    5771 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0119 13:40:09.936930    5771 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 13:40:10.129026    5771 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0119 13:40:10.188135    5771 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 13:40:10.484204    5771 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0119 13:40:11.119331    5771 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0119 13:40:11.119542    5771 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0119 13:40:11.134351    5771 start.go:540] Will wait 60s for crictl version
I0119 13:40:11.134499    5771 ssh_runner.go:195] Run: which crictl
I0119 13:40:11.147228    5771 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0119 13:40:11.576045    5771 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0119 13:40:11.576226    5771 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0119 13:40:11.832179    5771 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0119 13:40:11.973130    5771 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0119 13:40:11.975929    5771 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0119 13:40:12.157633    5771 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0119 13:40:12.173812    5771 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0119 13:40:12.233053    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0119 13:40:12.402204    5771 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0119 13:40:12.402379    5771 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0119 13:40:12.483635    5771 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I0119 13:40:12.483664    5771 docker.go:601] Images already preloaded, skipping extraction
I0119 13:40:12.483789    5771 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0119 13:40:12.553697    5771 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I0119 13:40:12.553731    5771 cache_images.go:84] Images are preloaded, skipping loading
I0119 13:40:12.553852    5771 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0119 13:40:13.074746    5771 cni.go:84] Creating CNI manager for ""
I0119 13:40:13.074787    5771 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0119 13:40:13.075379    5771 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0119 13:40:13.075507    5771 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0119 13:40:13.075943    5771 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0119 13:40:13.076207    5771 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0119 13:40:13.076374    5771 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0119 13:40:13.122421    5771 binaries.go:44] Found k8s binaries, skipping transfer
I0119 13:40:13.122631    5771 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0119 13:40:13.163525    5771 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0119 13:40:13.237923    5771 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0119 13:40:13.324610    5771 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2090 bytes)
I0119 13:40:13.429593    5771 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0119 13:40:13.463636    5771 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0119 13:40:13.556984    5771 certs.go:56] Setting up /home/nareash/.minikube/profiles/minikube for IP: 192.168.49.2
I0119 13:40:13.557067    5771 certs.go:190] acquiring lock for shared ca certs: {Name:mk11fb8cca897434c8a4c157c084ddddb1e3597e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 13:40:13.557735    5771 certs.go:199] skipping minikubeCA CA generation: /home/nareash/.minikube/ca.key
I0119 13:40:13.558238    5771 certs.go:199] skipping proxyClientCA CA generation: /home/nareash/.minikube/proxy-client-ca.key
I0119 13:40:13.558614    5771 certs.go:315] skipping minikube-user signed cert generation: /home/nareash/.minikube/profiles/minikube/client.key
I0119 13:40:13.559054    5771 certs.go:315] skipping minikube signed cert generation: /home/nareash/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0119 13:40:13.559534    5771 certs.go:315] skipping aggregator signed cert generation: /home/nareash/.minikube/profiles/minikube/proxy-client.key
I0119 13:40:13.560082    5771 certs.go:437] found cert: /home/nareash/.minikube/certs/home/nareash/.minikube/certs/ca-key.pem (1679 bytes)
I0119 13:40:13.560229    5771 certs.go:437] found cert: /home/nareash/.minikube/certs/home/nareash/.minikube/certs/ca.pem (1078 bytes)
I0119 13:40:13.560368    5771 certs.go:437] found cert: /home/nareash/.minikube/certs/home/nareash/.minikube/certs/cert.pem (1123 bytes)
I0119 13:40:13.560492    5771 certs.go:437] found cert: /home/nareash/.minikube/certs/home/nareash/.minikube/certs/key.pem (1675 bytes)
I0119 13:40:13.563305    5771 ssh_runner.go:362] scp /home/nareash/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0119 13:40:13.726451    5771 ssh_runner.go:362] scp /home/nareash/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0119 13:40:13.879882    5771 ssh_runner.go:362] scp /home/nareash/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0119 13:40:14.020591    5771 ssh_runner.go:362] scp /home/nareash/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0119 13:40:14.212872    5771 ssh_runner.go:362] scp /home/nareash/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0119 13:40:14.368027    5771 ssh_runner.go:362] scp /home/nareash/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0119 13:40:14.501198    5771 ssh_runner.go:362] scp /home/nareash/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0119 13:40:14.625504    5771 ssh_runner.go:362] scp /home/nareash/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0119 13:40:14.771331    5771 ssh_runner.go:362] scp /home/nareash/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0119 13:40:14.921546    5771 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (742 bytes)
I0119 13:40:15.038762    5771 ssh_runner.go:195] Run: openssl version
I0119 13:40:15.094601    5771 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0119 13:40:15.164563    5771 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0119 13:40:15.182262    5771 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Dec 12 09:23 /usr/share/ca-certificates/minikubeCA.pem
I0119 13:40:15.182434    5771 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0119 13:40:15.215671    5771 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0119 13:40:15.255119    5771 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0119 13:40:15.273559    5771 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0119 13:40:15.310533    5771 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0119 13:40:15.343469    5771 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0119 13:40:15.379770    5771 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0119 13:40:15.421311    5771 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0119 13:40:15.456090    5771 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0119 13:40:15.498736    5771 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3652 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/nareash:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0119 13:40:15.499198    5771 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0119 13:40:15.586746    5771 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0119 13:40:15.639187    5771 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0119 13:40:15.639215    5771 kubeadm.go:636] restartCluster start
I0119 13:40:15.639408    5771 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0119 13:40:15.698730    5771 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0119 13:40:15.698940    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0119 13:40:15.914543    5771 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:39759"
I0119 13:40:15.914593    5771 kubeconfig.go:135] verify returned: got: 127.0.0.1:39759, want: 127.0.0.1:44585
I0119 13:40:15.917351    5771 lock.go:35] WriteFile acquiring /home/nareash/.kube/config: {Name:mk9d6569051f2bcc9ff0b042a22d175449ea3530 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 13:40:15.929989    5771 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0119 13:40:15.971410    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:15.971563    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:16.027149    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:16.027179    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:16.027350    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:16.073524    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:16.574372    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:16.574621    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:16.618897    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:17.074367    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:17.074525    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:17.121641    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:17.574467    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:17.574660    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:17.621551    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:18.074015    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:18.074207    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:18.121647    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:18.574374    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:18.574605    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:18.636021    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:19.074466    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:19.074644    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:19.127985    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:19.574440    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:19.574644    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:19.621241    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:20.073986    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:20.074194    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:20.119275    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:20.573636    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:20.573865    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:20.620343    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:21.074490    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:21.074651    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:21.119507    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:21.574303    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:21.574479    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:21.622661    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:22.073848    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:22.074140    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:22.126134    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:22.574197    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:22.574333    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:22.622512    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:23.074440    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:23.074713    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:23.125458    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:23.574451    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:23.574688    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:23.662348    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:24.074041    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:24.074250    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:24.119327    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:24.574059    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:24.574255    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:24.634936    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:25.073942    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:25.074142    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:25.130188    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:25.574222    5771 api_server.go:166] Checking apiserver status ...
I0119 13:40:25.574414    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 13:40:25.639111    5771 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 13:40:25.971909    5771 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0119 13:40:25.971937    5771 kubeadm.go:1128] stopping kube-system containers ...
I0119 13:40:25.972062    5771 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0119 13:40:26.092289    5771 docker.go:469] Stopping containers: [b495c90b4806 292976a8f359 b22017608159 e8ede198286f 2ba7dca3e8aa 210711c39463 c167bfa5f62e 29ef249ddfa5 6cfee2c33ab9 8c45fc841aa0 118d7e578eba f9ec9facb88e 0b23518261ad 529a384d397f 47a3f563d55c]
I0119 13:40:26.092524    5771 ssh_runner.go:195] Run: docker stop b495c90b4806 292976a8f359 b22017608159 e8ede198286f 2ba7dca3e8aa 210711c39463 c167bfa5f62e 29ef249ddfa5 6cfee2c33ab9 8c45fc841aa0 118d7e578eba f9ec9facb88e 0b23518261ad 529a384d397f 47a3f563d55c
I0119 13:40:26.214289    5771 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0119 13:40:26.272783    5771 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0119 13:40:26.328448    5771 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Jan 19 07:52 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Jan 19 07:52 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Jan 19 07:53 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Jan 19 07:52 /etc/kubernetes/scheduler.conf

I0119 13:40:26.328657    5771 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0119 13:40:26.393696    5771 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0119 13:40:26.483193    5771 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0119 13:40:26.562607    5771 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0119 13:40:26.562839    5771 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0119 13:40:26.628584    5771 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0119 13:40:26.696096    5771 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0119 13:40:26.696317    5771 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0119 13:40:26.780477    5771 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0119 13:40:26.878794    5771 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0119 13:40:26.878836    5771 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0119 13:40:28.339442    5771 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (1.460547997s)
I0119 13:40:28.339492    5771 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0119 13:40:32.384942    5771 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (4.045397282s)
I0119 13:40:32.385015    5771 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0119 13:40:32.928767    5771 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0119 13:40:33.248373    5771 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0119 13:40:33.564595    5771 api_server.go:52] waiting for apiserver process to appear ...
I0119 13:40:33.564813    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 13:40:33.637405    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 13:40:34.237788    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 13:40:34.737120    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 13:40:35.237449    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 13:40:35.738015    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 13:40:36.237529    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 13:40:36.737590    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 13:40:37.241794    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 13:40:37.738118    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 13:40:38.237518    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 13:40:38.737744    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 13:40:38.823334    5771 api_server.go:72] duration metric: took 5.258741191s to wait for apiserver process to appear ...
I0119 13:40:38.823376    5771 api_server.go:88] waiting for apiserver healthz status ...
I0119 13:40:38.823435    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:38.830265    5771 api_server.go:269] stopped: https://127.0.0.1:44585/healthz: Get "https://127.0.0.1:44585/healthz": EOF
I0119 13:40:38.830322    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:38.838084    5771 api_server.go:269] stopped: https://127.0.0.1:44585/healthz: Get "https://127.0.0.1:44585/healthz": EOF
I0119 13:40:39.338671    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:39.352182    5771 api_server.go:269] stopped: https://127.0.0.1:44585/healthz: Get "https://127.0.0.1:44585/healthz": EOF
I0119 13:40:39.838580    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:39.848780    5771 api_server.go:269] stopped: https://127.0.0.1:44585/healthz: Get "https://127.0.0.1:44585/healthz": EOF
I0119 13:40:40.338570    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:40.343699    5771 api_server.go:269] stopped: https://127.0.0.1:44585/healthz: Get "https://127.0.0.1:44585/healthz": EOF
I0119 13:40:40.839199    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:45.840197    5771 api_server.go:269] stopped: https://127.0.0.1:44585/healthz: Get "https://127.0.0.1:44585/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0119 13:40:45.840259    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:47.710573    5771 api_server.go:279] https://127.0.0.1:44585/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0119 13:40:47.710619    5771 api_server.go:103] status: https://127.0.0.1:44585/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0119 13:40:47.710652    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:47.805962    5771 api_server.go:279] https://127.0.0.1:44585/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0119 13:40:47.806050    5771 api_server.go:103] status: https://127.0.0.1:44585/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0119 13:40:47.838282    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:47.876685    5771 api_server.go:279] https://127.0.0.1:44585/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0119 13:40:47.876740    5771 api_server.go:103] status: https://127.0.0.1:44585/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0119 13:40:48.338621    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:48.379474    5771 api_server.go:279] https://127.0.0.1:44585/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0119 13:40:48.379543    5771 api_server.go:103] status: https://127.0.0.1:44585/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0119 13:40:48.838777    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:48.884468    5771 api_server.go:279] https://127.0.0.1:44585/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0119 13:40:48.884539    5771 api_server.go:103] status: https://127.0.0.1:44585/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0119 13:40:49.339164    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:49.393598    5771 api_server.go:279] https://127.0.0.1:44585/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0119 13:40:49.393658    5771 api_server.go:103] status: https://127.0.0.1:44585/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0119 13:40:49.839246    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:49.876524    5771 api_server.go:279] https://127.0.0.1:44585/healthz returned 200:
ok
I0119 13:40:49.943114    5771 api_server.go:141] control plane version: v1.28.3
I0119 13:40:49.943160    5771 api_server.go:131] duration metric: took 11.119763846s to wait for apiserver health ...
I0119 13:40:49.943183    5771 cni.go:84] Creating CNI manager for ""
I0119 13:40:49.943233    5771 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0119 13:40:49.945383    5771 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0119 13:40:49.946610    5771 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0119 13:40:50.053330    5771 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0119 13:40:50.173096    5771 system_pods.go:43] waiting for kube-system pods to appear ...
I0119 13:40:50.237551    5771 system_pods.go:59] 7 kube-system pods found
I0119 13:40:50.237642    5771 system_pods.go:61] "coredns-5dd5756b68-q5kb4" [27cdc112-16c5-4676-8c3d-073a4b92a15c] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0119 13:40:50.237673    5771 system_pods.go:61] "etcd-minikube" [2f87a383-40f0-4aa4-9602-167dfa4bd731] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0119 13:40:50.237720    5771 system_pods.go:61] "kube-apiserver-minikube" [e1e2620c-8d68-4683-9fd0-874dd3aa8605] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0119 13:40:50.237751    5771 system_pods.go:61] "kube-controller-manager-minikube" [f936da13-efe3-4c32-b32b-77cdd0dcbb50] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0119 13:40:50.237776    5771 system_pods.go:61] "kube-proxy-svl6k" [ac20a2b9-08d1-452d-ad1e-723452d5058e] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0119 13:40:50.237802    5771 system_pods.go:61] "kube-scheduler-minikube" [f2298db4-4715-4380-866c-9df7ab056288] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0119 13:40:50.237827    5771 system_pods.go:61] "storage-provisioner" [7ad981d8-11a8-43e5-89ea-b05880e52202] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0119 13:40:50.237852    5771 system_pods.go:74] duration metric: took 64.727978ms to wait for pod list to return data ...
I0119 13:40:50.237872    5771 node_conditions.go:102] verifying NodePressure condition ...
I0119 13:40:50.258377    5771 node_conditions.go:122] node storage ephemeral capacity is 65739308Ki
I0119 13:40:50.258426    5771 node_conditions.go:123] node cpu capacity is 8
I0119 13:40:50.258486    5771 node_conditions.go:105] duration metric: took 20.598498ms to run NodePressure ...
I0119 13:40:50.258531    5771 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0119 13:40:51.369389    5771 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (1.110772926s)
I0119 13:40:51.369538    5771 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0119 13:40:51.414242    5771 ops.go:34] apiserver oom_adj: -16
I0119 13:40:51.414274    5771 kubeadm.go:640] restartCluster took 35.775041343s
I0119 13:40:51.414295    5771 kubeadm.go:406] StartCluster complete in 35.915575809s
I0119 13:40:51.414338    5771 settings.go:142] acquiring lock: {Name:mk5a73c2c95d845227a50b3f4a1dbc3e148d1595 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 13:40:51.414630    5771 settings.go:150] Updating kubeconfig:  /home/nareash/.kube/config
I0119 13:40:51.417056    5771 lock.go:35] WriteFile acquiring /home/nareash/.kube/config: {Name:mk9d6569051f2bcc9ff0b042a22d175449ea3530 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 13:40:51.423580    5771 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0119 13:40:51.423881    5771 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0119 13:40:51.424083    5771 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0119 13:40:51.424129    5771 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0119 13:40:51.424151    5771 addons.go:240] addon storage-provisioner should already be in state true
I0119 13:40:51.424152    5771 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0119 13:40:51.424302    5771 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0119 13:40:51.424341    5771 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0119 13:40:51.424900    5771 host.go:66] Checking if "minikube" exists ...
I0119 13:40:51.425308    5771 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 13:40:51.426516    5771 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 13:40:51.469256    5771 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0119 13:40:51.469352    5771 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0119 13:40:51.472056    5771 out.go:177] üîé  Verifying Kubernetes components...
I0119 13:40:51.474190    5771 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0119 13:40:51.767220    5771 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0119 13:40:51.767255    5771 addons.go:240] addon default-storageclass should already be in state true
I0119 13:40:51.767335    5771 host.go:66] Checking if "minikube" exists ...
I0119 13:40:51.768906    5771 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 13:40:51.821800    5771 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0119 13:40:51.823848    5771 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0119 13:40:51.823890    5771 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0119 13:40:51.824080    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 13:40:52.080155    5771 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0119 13:40:52.080188    5771 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0119 13:40:52.080363    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 13:40:52.081946    5771 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:43813 SSHKeyPath:/home/nareash/.minikube/machines/minikube/id_rsa Username:docker}
I0119 13:40:52.352472    5771 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:43813 SSHKeyPath:/home/nareash/.minikube/machines/minikube/id_rsa Username:docker}
I0119 13:40:52.440340    5771 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0119 13:40:52.637757    5771 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0119 13:40:52.675694    5771 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.252051249s)
I0119 13:40:52.675887    5771 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0119 13:40:52.675933    5771 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (1.201666159s)
I0119 13:40:52.676149    5771 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0119 13:40:52.866538    5771 api_server.go:52] waiting for apiserver process to appear ...
I0119 13:40:52.866687    5771 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 13:40:54.899078    5771 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.45868332s)
I0119 13:40:54.899287    5771 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.261480827s)
I0119 13:40:54.899456    5771 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.032729274s)
I0119 13:40:54.899489    5771 api_server.go:72] duration metric: took 3.43007109s to wait for apiserver process to appear ...
I0119 13:40:54.899502    5771 api_server.go:88] waiting for apiserver healthz status ...
I0119 13:40:54.899534    5771 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44585/healthz ...
I0119 13:40:54.926940    5771 api_server.go:279] https://127.0.0.1:44585/healthz returned 200:
ok
I0119 13:40:54.930038    5771 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0119 13:40:54.931019    5771 addons.go:502] enable addons completed in 3.507156757s: enabled=[storage-provisioner default-storageclass]
I0119 13:40:54.933243    5771 api_server.go:141] control plane version: v1.28.3
I0119 13:40:54.933281    5771 api_server.go:131] duration metric: took 33.76226ms to wait for apiserver health ...
I0119 13:40:54.933302    5771 system_pods.go:43] waiting for kube-system pods to appear ...
I0119 13:40:54.955570    5771 system_pods.go:59] 7 kube-system pods found
I0119 13:40:54.955604    5771 system_pods.go:61] "coredns-5dd5756b68-q5kb4" [27cdc112-16c5-4676-8c3d-073a4b92a15c] Running
I0119 13:40:54.955621    5771 system_pods.go:61] "etcd-minikube" [2f87a383-40f0-4aa4-9602-167dfa4bd731] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0119 13:40:54.955638    5771 system_pods.go:61] "kube-apiserver-minikube" [e1e2620c-8d68-4683-9fd0-874dd3aa8605] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0119 13:40:54.955679    5771 system_pods.go:61] "kube-controller-manager-minikube" [f936da13-efe3-4c32-b32b-77cdd0dcbb50] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0119 13:40:54.955707    5771 system_pods.go:61] "kube-proxy-svl6k" [ac20a2b9-08d1-452d-ad1e-723452d5058e] Running
I0119 13:40:54.955723    5771 system_pods.go:61] "kube-scheduler-minikube" [f2298db4-4715-4380-866c-9df7ab056288] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0119 13:40:54.955734    5771 system_pods.go:61] "storage-provisioner" [7ad981d8-11a8-43e5-89ea-b05880e52202] Running
I0119 13:40:54.955749    5771 system_pods.go:74] duration metric: took 22.433567ms to wait for pod list to return data ...
I0119 13:40:54.955783    5771 kubeadm.go:581] duration metric: took 3.486356007s to wait for : map[apiserver:true system_pods:true] ...
I0119 13:40:54.955811    5771 node_conditions.go:102] verifying NodePressure condition ...
I0119 13:40:54.967456    5771 node_conditions.go:122] node storage ephemeral capacity is 65739308Ki
I0119 13:40:54.967496    5771 node_conditions.go:123] node cpu capacity is 8
I0119 13:40:54.967525    5771 node_conditions.go:105] duration metric: took 11.701036ms to run NodePressure ...
I0119 13:40:54.967560    5771 start.go:228] waiting for startup goroutines ...
I0119 13:40:54.967598    5771 start.go:233] waiting for cluster config update ...
I0119 13:40:54.967631    5771 start.go:242] writing updated cluster config ...
I0119 13:40:54.968375    5771 ssh_runner.go:195] Run: rm -f paused
I0119 13:40:55.111895    5771 start.go:600] kubectl: 1.28.4, cluster: 1.28.3 (minor skew: 0)
I0119 13:40:55.113080    5771 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Jan 19 15:00:38 minikube cri-dockerd[1030]: time="2024-01-19T15:00:38Z" level=info msg="Pulling image wso2/wso2am:4.0.0: 829aace30071: Downloading [=======================================>           ]  153.8MB/195MB"
Jan 19 15:00:48 minikube cri-dockerd[1030]: time="2024-01-19T15:00:48Z" level=info msg="Pulling image wso2/wso2am:4.0.0: 829aace30071: Downloading [========================================>          ]  158.1MB/195MB"
Jan 19 15:00:58 minikube cri-dockerd[1030]: time="2024-01-19T15:00:58Z" level=info msg="Pulling image wso2/wso2am:4.0.0: 829aace30071: Downloading [==========================================>        ]  167.3MB/195MB"
Jan 19 15:01:08 minikube cri-dockerd[1030]: time="2024-01-19T15:01:08Z" level=info msg="Pulling image wso2/wso2am:4.0.0: 829aace30071: Downloading [==============================================>    ]  181.3MB/195MB"
Jan 19 15:01:18 minikube cri-dockerd[1030]: time="2024-01-19T15:01:18Z" level=info msg="Pulling image wso2/wso2am:4.0.0: 829aace30071: Downloading [================================================>  ]  187.7MB/195MB"
Jan 19 15:01:28 minikube cri-dockerd[1030]: time="2024-01-19T15:01:28Z" level=info msg="Pulling image wso2/wso2am:4.0.0: 829aace30071: Extracting [================>                                  ]  66.29MB/195MB"
Jan 19 15:01:38 minikube cri-dockerd[1030]: time="2024-01-19T15:01:38Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [============>                                      ]  102.9MB/421.2MB"
Jan 19 15:01:48 minikube cri-dockerd[1030]: time="2024-01-19T15:01:48Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [===============>                                   ]  127.2MB/421.2MB"
Jan 19 15:01:58 minikube cri-dockerd[1030]: time="2024-01-19T15:01:58Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [=================>                                 ]    145MB/421.2MB"
Jan 19 15:02:08 minikube cri-dockerd[1030]: time="2024-01-19T15:02:08Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [==================>                                ]  154.7MB/421.2MB"
Jan 19 15:02:18 minikube cri-dockerd[1030]: time="2024-01-19T15:02:18Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [===================>                               ]  163.9MB/421.2MB"
Jan 19 15:02:25 minikube cri-dockerd[1030]: time="2024-01-19T15:02:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d6bb62e6840d8775bb47b94da0cafcf25bc39ac5e0c99c457c3cc1a0e2649579/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 19 15:02:28 minikube cri-dockerd[1030]: time="2024-01-19T15:02:28Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [====================>                              ]  170.4MB/421.2MB"
Jan 19 15:02:38 minikube cri-dockerd[1030]: time="2024-01-19T15:02:38Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [=====================>                             ]  180.1MB/421.2MB"
Jan 19 15:02:48 minikube cri-dockerd[1030]: time="2024-01-19T15:02:48Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [======================>                            ]  190.9MB/421.2MB"
Jan 19 15:02:58 minikube cri-dockerd[1030]: time="2024-01-19T15:02:58Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [========================>                          ]  202.2MB/421.2MB"
Jan 19 15:03:08 minikube cri-dockerd[1030]: time="2024-01-19T15:03:08Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [=========================>                         ]  217.3MB/421.2MB"
Jan 19 15:03:18 minikube cri-dockerd[1030]: time="2024-01-19T15:03:18Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [============================>                      ]  238.4MB/421.2MB"
Jan 19 15:03:28 minikube cri-dockerd[1030]: time="2024-01-19T15:03:28Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [=============================>                     ]  252.4MB/421.2MB"
Jan 19 15:03:38 minikube cri-dockerd[1030]: time="2024-01-19T15:03:38Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [===============================>                   ]  265.4MB/421.2MB"
Jan 19 15:03:48 minikube cri-dockerd[1030]: time="2024-01-19T15:03:48Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [================================>                  ]  271.3MB/421.2MB"
Jan 19 15:03:58 minikube cri-dockerd[1030]: time="2024-01-19T15:03:58Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [=================================>                 ]  280.5MB/421.2MB"
Jan 19 15:04:08 minikube cri-dockerd[1030]: time="2024-01-19T15:04:08Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [==================================>                ]  290.2MB/421.2MB"
Jan 19 15:04:18 minikube cri-dockerd[1030]: time="2024-01-19T15:04:18Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [===================================>               ]  297.2MB/421.2MB"
Jan 19 15:04:28 minikube cri-dockerd[1030]: time="2024-01-19T15:04:28Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [====================================>              ]  305.9MB/421.2MB"
Jan 19 15:04:38 minikube cri-dockerd[1030]: time="2024-01-19T15:04:38Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [=====================================>             ]    315MB/421.2MB"
Jan 19 15:04:48 minikube cri-dockerd[1030]: time="2024-01-19T15:04:48Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [======================================>            ]  326.3MB/421.2MB"
Jan 19 15:04:58 minikube cri-dockerd[1030]: time="2024-01-19T15:04:58Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [=======================================>           ]    335MB/421.2MB"
Jan 19 15:05:08 minikube cri-dockerd[1030]: time="2024-01-19T15:05:08Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [========================================>          ]  343.6MB/421.2MB"
Jan 19 15:05:18 minikube cri-dockerd[1030]: time="2024-01-19T15:05:18Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [=========================================>         ]  352.8MB/421.2MB"
Jan 19 15:05:28 minikube cri-dockerd[1030]: time="2024-01-19T15:05:28Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [==========================================>        ]  359.2MB/421.2MB"
Jan 19 15:05:38 minikube cri-dockerd[1030]: time="2024-01-19T15:05:38Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [===========================================>       ]  365.7MB/421.2MB"
Jan 19 15:05:48 minikube cri-dockerd[1030]: time="2024-01-19T15:05:48Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [============================================>      ]  372.7MB/421.2MB"
Jan 19 15:05:58 minikube cri-dockerd[1030]: time="2024-01-19T15:05:58Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [============================================>      ]  378.1MB/421.2MB"
Jan 19 15:06:08 minikube cri-dockerd[1030]: time="2024-01-19T15:06:08Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [=============================================>     ]  384.6MB/421.2MB"
Jan 19 15:06:18 minikube cri-dockerd[1030]: time="2024-01-19T15:06:18Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [==============================================>    ]  391.1MB/421.2MB"
Jan 19 15:06:28 minikube cri-dockerd[1030]: time="2024-01-19T15:06:28Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [==============================================>    ]  395.4MB/421.2MB"
Jan 19 15:06:38 minikube cri-dockerd[1030]: time="2024-01-19T15:06:38Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [================================================>  ]  405.6MB/421.2MB"
Jan 19 15:06:48 minikube cri-dockerd[1030]: time="2024-01-19T15:06:48Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [================================================>  ]    411MB/421.2MB"
Jan 19 15:06:58 minikube cri-dockerd[1030]: time="2024-01-19T15:06:58Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Downloading [=================================================> ]  415.9MB/421.2MB"
Jan 19 15:07:08 minikube cri-dockerd[1030]: time="2024-01-19T15:07:08Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Extracting [===========>                                       ]  100.8MB/421.2MB"
Jan 19 15:07:18 minikube cri-dockerd[1030]: time="2024-01-19T15:07:18Z" level=info msg="Pulling image wso2/wso2am:4.0.0: c5af8a3e0e8a: Extracting [============================================>      ]  373.8MB/421.2MB"
Jan 19 15:07:24 minikube cri-dockerd[1030]: time="2024-01-19T15:07:24Z" level=info msg="Stop pulling image wso2/wso2am:4.0.0: Status: Downloaded newer image for wso2/wso2am:4.0.0"
Jan 19 15:07:31 minikube cri-dockerd[1030]: time="2024-01-19T15:07:31Z" level=info msg="Stop pulling image wso2/wso2am:4.0.0: Status: Image is up to date for wso2/wso2am:4.0.0"
Jan 19 15:07:55 minikube dockerd[797]: time="2024-01-19T15:07:55.486904998Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=148da073e3b42537bf6c2759a8fc83fba5289937383d8ac402384179a8f83059
Jan 19 15:07:55 minikube dockerd[797]: time="2024-01-19T15:07:55.841106440Z" level=info msg="ignoring event" container=148da073e3b42537bf6c2759a8fc83fba5289937383d8ac402384179a8f83059 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 15:07:56 minikube cri-dockerd[1030]: time="2024-01-19T15:07:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"apim-deployment-7b8dcdb9d8-xxfxp_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Jan 19 15:07:56 minikube dockerd[797]: time="2024-01-19T15:07:56.688097884Z" level=info msg="ignoring event" container=300707ef9a828012c3227484a787d327e1239c60dadd0f5ea85d36dbce517c17 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 15:27:47 minikube cri-dockerd[1030]: time="2024-01-19T15:27:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8fdfb4990c2c11e38a496457baba3ec92be2e11b06c75ced28ef28e70cafaae1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 19 15:28:18 minikube dockerd[797]: time="2024-01-19T15:28:18.456056423Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=6a573a92d44c198fb7a5cdec2a38c4cb5b2b186be1c45990517262a3123b3ed1
Jan 19 15:28:20 minikube dockerd[797]: time="2024-01-19T15:28:20.081686355Z" level=info msg="ignoring event" container=6a573a92d44c198fb7a5cdec2a38c4cb5b2b186be1c45990517262a3123b3ed1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 15:28:22 minikube dockerd[797]: time="2024-01-19T15:28:22.359283672Z" level=info msg="ignoring event" container=d6bb62e6840d8775bb47b94da0cafcf25bc39ac5e0c99c457c3cc1a0e2649579 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 16:07:04 minikube cri-dockerd[1030]: time="2024-01-19T16:07:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3f28ecb3bb8b58a9782efc06b5570ff2ab1adb55a229c37369fc575e5350ff6d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 19 16:07:27 minikube cri-dockerd[1030]: time="2024-01-19T16:07:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e62042f58c2f7952874196ca02072f8920c63c4b09481bcec7b50141cf5f3772/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 19 16:07:32 minikube dockerd[797]: time="2024-01-19T16:07:32.984682959Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=0088bdedd4a732b33579445acd0626557f09873d67127cd308e4bd6333ef4c97
Jan 19 16:07:34 minikube dockerd[797]: time="2024-01-19T16:07:34.255060714Z" level=info msg="ignoring event" container=0088bdedd4a732b33579445acd0626557f09873d67127cd308e4bd6333ef4c97 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 16:07:35 minikube dockerd[797]: time="2024-01-19T16:07:35.546753491Z" level=info msg="ignoring event" container=8fdfb4990c2c11e38a496457baba3ec92be2e11b06c75ced28ef28e70cafaae1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 16:07:56 minikube dockerd[797]: time="2024-01-19T16:07:56.003868401Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=a90edb494cb8b1615db30a3b9843f7a9879b2a0971f7909708c93834203853f5
Jan 19 16:07:56 minikube dockerd[797]: time="2024-01-19T16:07:56.369964623Z" level=info msg="ignoring event" container=a90edb494cb8b1615db30a3b9843f7a9879b2a0971f7909708c93834203853f5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 16:07:56 minikube dockerd[797]: time="2024-01-19T16:07:56.723125857Z" level=info msg="ignoring event" container=3f28ecb3bb8b58a9782efc06b5570ff2ab1adb55a229c37369fc575e5350ff6d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
40c256975b32b       4de4fa31ea222                                                                   8 minutes ago       Running             apim                      0                   e62042f58c2f7       apim-deployment-7dcbfb86b4-8zxvd
7f37f6755d640       nginx@sha256:401bc2c812345a6c64a0fd5504db5e8b511c7c8e62855a827c57944280835703   7 hours ago         Running             nginx                     0                   9b6c89a74f60f       nginx
6866be2b7491c       6e38f40d628db                                                                   8 hours ago         Running             storage-provisioner       3                   5e70aa70b8148       storage-provisioner
c459d6bd9c734       ead0a4a53df89                                                                   8 hours ago         Running             coredns                   1                   08013dfe5d2d0       coredns-5dd5756b68-q5kb4
8662f984cc4fd       6e38f40d628db                                                                   8 hours ago         Exited              storage-provisioner       2                   5e70aa70b8148       storage-provisioner
2fc8f7ff5b553       bfc896cf80fba                                                                   8 hours ago         Running             kube-proxy                1                   afcca9dfe6b27       kube-proxy-svl6k
2b3e02024c60b       6d1b4fd1b182d                                                                   8 hours ago         Running             kube-scheduler            1                   8af756097b240       kube-scheduler-minikube
9eb5818d683ff       73deb9a3f7025                                                                   8 hours ago         Running             etcd                      1                   6987a3c597e37       etcd-minikube
a355c64d047b7       5374347291230                                                                   8 hours ago         Running             kube-apiserver            1                   03fbe098bf02e       kube-apiserver-minikube
e432b83e9608f       10baa1ca17068                                                                   8 hours ago         Running             kube-controller-manager   1                   d033bf5b553c4       kube-controller-manager-minikube
292976a8f3598       ead0a4a53df89                                                                   8 hours ago         Exited              coredns                   0                   b22017608159b       coredns-5dd5756b68-q5kb4
2ba7dca3e8aa7       bfc896cf80fba                                                                   8 hours ago         Exited              kube-proxy                0                   c167bfa5f62e3       kube-proxy-svl6k
29ef249ddfa56       6d1b4fd1b182d                                                                   8 hours ago         Exited              kube-scheduler            0                   f9ec9facb88e6       kube-scheduler-minikube
6cfee2c33ab93       73deb9a3f7025                                                                   8 hours ago         Exited              etcd                      0                   47a3f563d55c5       etcd-minikube
8c45fc841aa06       5374347291230                                                                   8 hours ago         Exited              kube-apiserver            0                   529a384d397f0       kube-apiserver-minikube
118d7e578eba8       10baa1ca17068                                                                   8 hours ago         Exited              kube-controller-manager   0                   0b23518261ad6       kube-controller-manager-minikube

* 
* ==> coredns [292976a8f359] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:55052 - 22022 "HINFO IN 1329536239499144772.1873764344536711800. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.048539905s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [c459d6bd9c73] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:54870 - 7132 "HINFO IN 4401498448336275627.1608899300060660119. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.023319321s
[INFO] 10.244.0.9:43731 - 20095 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.110376703s
[INFO] 10.244.0.9:58559 - 28880 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.007758807s
[INFO] 10.244.0.9:40438 - 9588 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.01078305s
[INFO] 10.244.0.9:51641 - 3219 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.007855451s
[INFO] 10.244.0.9:40506 - 58636 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.007000586s
[INFO] 10.244.0.9:35988 - 28971 "A IN api.updates.wso2.com.default.svc.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.011199142s
[INFO] 10.244.0.9:37150 - 36435 "A IN api.updates.wso2.com.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.013502912s
[INFO] 10.244.0.9:55340 - 25150 "A IN api.updates.wso2.com.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000457389s
[INFO] 10.244.0.9:34621 - 59400 "A IN api.updates.wso2.com. udp 38 false 512" NOERROR qr,rd,ra 110 0.140126808s
[INFO] 10.244.0.9:60057 - 20192 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.011464031s
[INFO] 10.244.0.9:46787 - 4804 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.011324947s
[INFO] 10.244.0.10:33425 - 49776 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.068329079s
[INFO] 10.244.0.10:48048 - 28156 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.005985291s
[INFO] 10.244.0.10:53851 - 52103 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.00871832s
[INFO] 10.244.0.10:48470 - 62224 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.009268485s
[INFO] 10.244.0.10:40146 - 39836 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.014996187s
[INFO] 10.244.0.10:36695 - 48576 "A IN api.updates.wso2.com.default.svc.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.006535336s
[INFO] 10.244.0.10:56401 - 58286 "A IN api.updates.wso2.com.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.000570521s
[INFO] 10.244.0.10:58757 - 39001 "A IN api.updates.wso2.com.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000560815s
[INFO] 10.244.0.10:52091 - 6306 "A IN api.updates.wso2.com. udp 38 false 512" NOERROR qr,rd,ra 110 1.706740935s
[INFO] 10.244.0.10:57022 - 16781 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.022911661s
[INFO] 10.244.0.10:48302 - 62499 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.018459258s
[INFO] 10.244.0.12:54395 - 25963 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.124949636s
[INFO] 10.244.0.12:39610 - 19797 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.00745706s
[INFO] 10.244.0.12:53050 - 21925 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.033018155s
[INFO] 10.244.0.12:48872 - 39231 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.012370904s
[INFO] 10.244.0.12:42847 - 6644 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.010488349s
[INFO] 10.244.0.12:47327 - 33370 "A IN api.updates.wso2.com.default.svc.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.007175934s
[INFO] 10.244.0.12:36103 - 60834 "A IN api.updates.wso2.com.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.001722457s
[INFO] 10.244.0.12:60886 - 6035 "A IN api.updates.wso2.com.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000974741s
[INFO] 10.244.0.12:49596 - 36859 "A IN api.updates.wso2.com. udp 38 false 512" NOERROR qr,rd,ra 110 5.1539900339999996s
[INFO] 10.244.0.12:49596 - 36859 "A IN api.updates.wso2.com. udp 38 false 512" NOERROR qr,rd,ra 110 0.152533112s
[INFO] 10.244.0.12:53001 - 27415 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.025154231s
[INFO] 10.244.0.12:51481 - 28169 "PTR IN 0.0.0.0.in-addr.arpa. udp 38 false 512" NXDOMAIN qr,rd,ra 38 0.022310374s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_01_19T13_23_02_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 19 Jan 2024 07:52:57 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 19 Jan 2024 16:15:29 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 19 Jan 2024 16:14:21 +0000   Fri, 19 Jan 2024 14:47:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 19 Jan 2024 16:14:21 +0000   Fri, 19 Jan 2024 14:47:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 19 Jan 2024 16:14:21 +0000   Fri, 19 Jan 2024 14:47:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 19 Jan 2024 16:14:21 +0000   Fri, 19 Jan 2024 14:47:19 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  65739308Ki
  memory:             3789620Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  65739308Ki
  memory:             3789620Ki
  pods:               110
System Info:
  Machine ID:                 a242c14755de4aeaa0617ebe7eb33119
  System UUID:                a242c14755de4aeaa0617ebe7eb33119
  Boot ID:                    9e9369da-b106-4b49-8a6a-66a93d6581ac
  Kernel Version:             6.5.11-linuxkit
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     apim-deployment-7dcbfb86b4-8zxvd    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8m8s
  default                     nginx                               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7h1m
  kube-system                 coredns-5dd5756b68-q5kb4            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     8h
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         8h
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8h
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8h
  kube-system                 kube-proxy-svl6k                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8h
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8h
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (4%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Jan19 08:09] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.115359] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.674488] netlink: 'init': attribute type 4 has an invalid length.
[  +5.806705] grpcfuse: loading out-of-tree module taints kernel.
[ +13.527059] hrtimer: interrupt took 2059099 ns
[Jan19 08:10] systemd[2130]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[Jan19 14:47] Hangcheck: hangcheck value past margin!
[Jan19 14:51] Hangcheck: hangcheck value past margin!

* 
* ==> etcd [6cfee2c33ab9] <==
* {"level":"warn","ts":"2024-01-19T07:52:50.338502Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-01-19T07:52:50.341465Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-01-19T07:52:50.342125Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-01-19T07:52:50.342272Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-01-19T07:52:50.342668Z","caller":"embed/etcd.go:495","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-19T07:52:50.346482Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-01-19T07:52:50.347443Z","caller":"embed/etcd.go:309","msg":"starting an etcd server","etcd-version":"3.5.9","git-sha":"bdbbde998","go-version":"go1.19.9","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-01-19T07:52:50.356632Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"7.963462ms"}
{"level":"info","ts":"2024-01-19T07:52:50.371369Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2024-01-19T07:52:50.372815Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-01-19T07:52:50.37303Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2024-01-19T07:52:50.373163Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-01-19T07:52:50.373207Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2024-01-19T07:52:50.373412Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2024-01-19T07:52:50.383231Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-01-19T07:52:50.395862Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-01-19T07:52:50.399425Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-01-19T07:52:50.409983Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-01-19T07:52:50.412466Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-01-19T07:52:50.412311Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-19T07:52:50.41376Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-19T07:52:50.413843Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-19T07:52:50.415267Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-01-19T07:52:50.415749Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-01-19T07:52:50.424389Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-19T07:52:50.424664Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-19T07:52:50.425759Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-01-19T07:52:50.425963Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-01-19T07:52:50.426842Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-19T07:52:50.675318Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2024-01-19T07:52:50.675501Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2024-01-19T07:52:50.675662Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2024-01-19T07:52:50.675786Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2024-01-19T07:52:50.675881Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-01-19T07:52:50.675936Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2024-01-19T07:52:50.675979Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-01-19T07:52:50.678992Z","caller":"etcdserver/server.go:2571","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-19T07:52:50.681627Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-19T07:52:50.682437Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-19T07:52:50.682301Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-19T07:52:50.683464Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-19T07:52:50.683829Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-19T07:52:50.683966Z","caller":"etcdserver/server.go:2595","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-19T07:52:50.684001Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-19T07:52:50.68413Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-01-19T07:52:50.691942Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-01-19T07:52:50.693907Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-01-19T08:02:25.059663Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-01-19T08:02:25.085511Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-01-19T08:02:25.115701Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-01-19T08:02:25.129834Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-01-19T08:02:25.257407Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-01-19T08:02:25.259141Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-01-19T08:02:25.261951Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-01-19T08:02:25.286251Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-19T08:02:25.286784Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-19T08:02:25.286853Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [9eb5818d683f] <==
* {"level":"info","ts":"2024-01-19T14:57:08.632097Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12437}
{"level":"info","ts":"2024-01-19T14:57:08.634913Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":12437,"took":"2.208418ms","hash":1031430854}
{"level":"info","ts":"2024-01-19T14:57:08.635031Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1031430854,"revision":12437,"compact-revision":12146}
{"level":"info","ts":"2024-01-19T15:01:30.949749Z","caller":"traceutil/trace.go:171","msg":"trace[1228956468] transaction","detail":"{read_only:false; response_revision:12902; number_of_response:1; }","duration":"108.782502ms","start":"2024-01-19T15:01:30.840905Z","end":"2024-01-19T15:01:30.949687Z","steps":["trace[1228956468] 'process raft request'  (duration: 108.418005ms)"],"step_count":1}
{"level":"info","ts":"2024-01-19T15:02:05.43274Z","caller":"traceutil/trace.go:171","msg":"trace[2014756557] transaction","detail":"{read_only:false; response_revision:12928; number_of_response:1; }","duration":"104.503449ms","start":"2024-01-19T15:02:05.328186Z","end":"2024-01-19T15:02:05.432689Z","steps":["trace[2014756557] 'process raft request'  (duration: 104.248801ms)"],"step_count":1}
{"level":"info","ts":"2024-01-19T15:02:08.649458Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12693}
{"level":"info","ts":"2024-01-19T15:02:08.719812Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":12693,"took":"69.720216ms","hash":2899179547}
{"level":"info","ts":"2024-01-19T15:02:08.719949Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2899179547,"revision":12693,"compact-revision":12437}
{"level":"info","ts":"2024-01-19T15:07:08.665261Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12932}
{"level":"info","ts":"2024-01-19T15:07:08.704523Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":12932,"took":"38.640679ms","hash":30777660}
{"level":"info","ts":"2024-01-19T15:07:08.704616Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":30777660,"revision":12932,"compact-revision":12693}
{"level":"info","ts":"2024-01-19T15:07:15.092528Z","caller":"traceutil/trace.go:171","msg":"trace[1711021504] linearizableReadLoop","detail":"{readStateIndex:16443; appliedIndex:16442; }","duration":"143.575421ms","start":"2024-01-19T15:07:14.948902Z","end":"2024-01-19T15:07:15.092478Z","steps":["trace[1711021504] 'read index received'  (duration: 143.096072ms)","trace[1711021504] 'applied index is now lower than readState.Index'  (duration: 474.583¬µs)"],"step_count":2}
{"level":"info","ts":"2024-01-19T15:07:15.092702Z","caller":"traceutil/trace.go:171","msg":"trace[210233795] transaction","detail":"{read_only:false; response_revision:13200; number_of_response:1; }","duration":"182.937146ms","start":"2024-01-19T15:07:14.909728Z","end":"2024-01-19T15:07:15.092664Z","steps":["trace[210233795] 'process raft request'  (duration: 182.425757ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T15:07:15.092904Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"144.05397ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2024-01-19T15:07:15.093103Z","caller":"traceutil/trace.go:171","msg":"trace[183402492] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:13200; }","duration":"144.301375ms","start":"2024-01-19T15:07:14.948761Z","end":"2024-01-19T15:07:15.093063Z","steps":["trace[183402492] 'agreement among raft nodes before linearized reading'  (duration: 143.901358ms)"],"step_count":1}
{"level":"info","ts":"2024-01-19T15:08:27.647806Z","caller":"traceutil/trace.go:171","msg":"trace[673292364] transaction","detail":"{read_only:false; response_revision:13272; number_of_response:1; }","duration":"124.570107ms","start":"2024-01-19T15:08:27.523178Z","end":"2024-01-19T15:08:27.647748Z","steps":["trace[673292364] 'process raft request'  (duration: 124.268847ms)"],"step_count":1}
{"level":"info","ts":"2024-01-19T15:12:08.707869Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13196}
{"level":"info","ts":"2024-01-19T15:12:09.166543Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":13196,"took":"414.708568ms","hash":3201628673}
{"level":"info","ts":"2024-01-19T15:12:09.16725Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3201628673,"revision":13196,"compact-revision":12932}
{"level":"info","ts":"2024-01-19T15:17:08.75542Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13447}
{"level":"info","ts":"2024-01-19T15:17:08.780495Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":13447,"took":"23.282429ms","hash":2332401699}
{"level":"info","ts":"2024-01-19T15:17:08.780607Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2332401699,"revision":13447,"compact-revision":13196}
{"level":"info","ts":"2024-01-19T15:22:08.52993Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13685}
{"level":"info","ts":"2024-01-19T15:22:08.539027Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":13685,"took":"8.526461ms","hash":2287615868}
{"level":"info","ts":"2024-01-19T15:22:08.539143Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2287615868,"revision":13685,"compact-revision":13447}
{"level":"info","ts":"2024-01-19T15:27:08.560559Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13925}
{"level":"info","ts":"2024-01-19T15:27:08.58233Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":13925,"took":"21.297609ms","hash":2265077422}
{"level":"info","ts":"2024-01-19T15:27:08.582414Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2265077422,"revision":13925,"compact-revision":13685}
{"level":"warn","ts":"2024-01-19T15:28:58.21569Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"119.531347ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026599391074125 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:14281 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128026599391074123 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-19T15:28:58.232007Z","caller":"traceutil/trace.go:171","msg":"trace[420341770] transaction","detail":"{read_only:false; response_revision:14289; number_of_response:1; }","duration":"174.918757ms","start":"2024-01-19T15:28:58.051548Z","end":"2024-01-19T15:28:58.226467Z","steps":["trace[420341770] 'store kv pair into bolt db' {req_type:put; key:/registry/masterleases/192.168.49.2; req_size:115; } (duration: 91.319011ms)"],"step_count":1}
{"level":"info","ts":"2024-01-19T15:32:08.575828Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14170}
{"level":"info","ts":"2024-01-19T15:32:08.663691Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":14170,"took":"85.057868ms","hash":2040766215}
{"level":"info","ts":"2024-01-19T15:32:08.664562Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2040766215,"revision":14170,"compact-revision":13925}
{"level":"info","ts":"2024-01-19T15:37:08.575155Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14442}
{"level":"info","ts":"2024-01-19T15:37:08.690149Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":14442,"took":"113.169995ms","hash":4087933234}
{"level":"info","ts":"2024-01-19T15:37:08.690276Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4087933234,"revision":14442,"compact-revision":14170}
{"level":"info","ts":"2024-01-19T15:42:08.591865Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14688}
{"level":"info","ts":"2024-01-19T15:42:08.60513Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":14688,"took":"12.627201ms","hash":1485637582}
{"level":"info","ts":"2024-01-19T15:42:08.606161Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1485637582,"revision":14688,"compact-revision":14442}
{"level":"info","ts":"2024-01-19T15:47:08.586629Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14929}
{"level":"info","ts":"2024-01-19T15:47:08.588413Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":14929,"took":"1.353013ms","hash":4025509373}
{"level":"info","ts":"2024-01-19T15:47:08.588486Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4025509373,"revision":14929,"compact-revision":14688}
{"level":"info","ts":"2024-01-19T15:52:08.57328Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15176}
{"level":"info","ts":"2024-01-19T15:52:08.575835Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15176,"took":"2.118607ms","hash":3796933638}
{"level":"info","ts":"2024-01-19T15:52:08.575919Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3796933638,"revision":15176,"compact-revision":14929}
{"level":"info","ts":"2024-01-19T15:57:08.608656Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15418}
{"level":"info","ts":"2024-01-19T15:57:08.613198Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15418,"took":"3.805887ms","hash":3342192958}
{"level":"info","ts":"2024-01-19T15:57:08.613413Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3342192958,"revision":15418,"compact-revision":15176}
{"level":"info","ts":"2024-01-19T16:02:08.64913Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15656}
{"level":"info","ts":"2024-01-19T16:02:08.650922Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15656,"took":"1.377123ms","hash":1789669948}
{"level":"info","ts":"2024-01-19T16:02:08.650991Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1789669948,"revision":15656,"compact-revision":15418}
{"level":"info","ts":"2024-01-19T16:05:00.068518Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-01-19T16:05:00.09242Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2024-01-19T16:05:00.093721Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":15002}
{"level":"info","ts":"2024-01-19T16:07:08.675868Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15906}
{"level":"info","ts":"2024-01-19T16:07:08.688212Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15906,"took":"11.529972ms","hash":3451791952}
{"level":"info","ts":"2024-01-19T16:07:08.688351Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3451791952,"revision":15906,"compact-revision":15656}
{"level":"info","ts":"2024-01-19T16:12:08.748981Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16187}
{"level":"info","ts":"2024-01-19T16:12:09.029855Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16187,"took":"246.325752ms","hash":1022460545}
{"level":"info","ts":"2024-01-19T16:12:09.030056Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1022460545,"revision":16187,"compact-revision":15906}

* 
* ==> kernel <==
*  16:15:33 up  8:06,  0 users,  load average: 1.24, 3.50, 2.74
Linux minikube 6.5.11-linuxkit #1 SMP PREEMPT_DYNAMIC Mon Dec  4 10:03:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [8c45fc841aa0] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0119 08:02:34.311825       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0119 08:02:34.420381       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0119 08:02:34.425833       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0119 08:02:34.435810       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0119 08:02:34.477325       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0119 08:02:34.477494       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0119 08:02:34.486619       1 logging.go:59] [core] [Channel #10 SubChannel #11] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-apiserver [a355c64d047b] <==
* I0119 08:10:47.510928       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0119 08:10:47.511003       1 controller.go:78] Starting OpenAPI AggregationController
I0119 08:10:47.511049       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0119 08:10:47.511103       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0119 08:10:47.511190       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0119 08:10:47.511122       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0119 08:10:47.511613       1 aggregator.go:164] waiting for initial CRD sync...
I0119 08:10:47.511629       1 available_controller.go:423] Starting AvailableConditionController
I0119 08:10:47.511667       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0119 08:10:47.511812       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0119 08:10:47.511857       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0119 08:10:47.512111       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0119 08:10:47.512154       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0119 08:10:47.512294       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0119 08:10:47.512790       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0119 08:10:47.512919       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0119 08:10:47.513449       1 controller.go:116] Starting legacy_token_tracking_controller
I0119 08:10:47.513557       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0119 08:10:47.514173       1 controller.go:134] Starting OpenAPI controller
I0119 08:10:47.514345       1 controller.go:85] Starting OpenAPI V3 controller
I0119 08:10:47.514501       1 naming_controller.go:291] Starting NamingConditionController
I0119 08:10:47.514627       1 establishing_controller.go:76] Starting EstablishingController
I0119 08:10:47.514729       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0119 08:10:47.514917       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0119 08:10:47.512423       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0119 08:10:47.515011       1 crd_finalizer.go:266] Starting CRDFinalizer
I0119 08:10:47.512508       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0119 08:10:47.524195       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0119 08:10:47.524556       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I0119 08:10:47.690429       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0119 08:10:47.711822       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0119 08:10:47.716965       1 shared_informer.go:318] Caches are synced for configmaps
I0119 08:10:47.719167       1 shared_informer.go:318] Caches are synced for node_authorizer
I0119 08:10:47.725118       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0119 08:10:47.725160       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0119 08:10:47.732121       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0119 08:10:47.732255       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0119 08:10:47.742073       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0119 08:10:47.736277       1 aggregator.go:166] initial CRD sync complete...
I0119 08:10:47.751953       1 autoregister_controller.go:141] Starting autoregister controller
I0119 08:10:47.752032       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0119 08:10:47.752967       1 cache.go:39] Caches are synced for autoregister controller
E0119 08:10:47.786758       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0119 08:10:48.531182       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0119 08:10:50.827161       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0119 08:10:50.907802       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0119 08:10:51.120908       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0119 08:10:51.240855       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0119 08:10:51.271247       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0119 08:10:54.834094       1 controller.go:624] quota admission added evaluator for: endpoints
I0119 08:11:01.855148       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0119 08:18:30.732100       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0119 08:21:31.403035       1 alloc.go:330] "allocated clusterIPs" service="default/nginx-service" clusterIPs={"IPv4":"10.96.211.59"}
I0119 09:14:06.215816       1 alloc.go:330] "allocated clusterIPs" service="default/nginx-service" clusterIPs={"IPv4":"10.106.73.170"}
I0119 14:47:08.605991       1 trace.go:236] Trace[995434180]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (19-Jan-2024 14:47:07.923) (total time: 673ms):
Trace[995434180]: ---"initial value restored" 567ms (14:47:08.500)
Trace[995434180]: ---"Transaction prepared" 82ms (14:47:08.582)
Trace[995434180]: [673.523515ms] [673.523515ms] END
E0119 14:47:18.745318       1 controller.go:193] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: ca369e37-8aaa-4136-8b7c-dd61315f9d10, UID in object meta: "
I0119 15:02:24.946827       1 alloc.go:330] "allocated clusterIPs" service="default/apim-service" clusterIPs={"IPv4":"10.103.138.189"}

* 
* ==> kube-controller-manager [118d7e578eba] <==
* I0119 07:53:14.348544       1 shared_informer.go:318] Caches are synced for persistent volume
I0119 07:53:14.348938       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0119 07:53:14.349215       1 shared_informer.go:318] Caches are synced for ephemeral
I0119 07:53:14.349848       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0119 07:53:14.356798       1 shared_informer.go:318] Caches are synced for ReplicationController
I0119 07:53:14.357606       1 shared_informer.go:318] Caches are synced for cronjob
I0119 07:53:14.361485       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0119 07:53:14.363949       1 shared_informer.go:318] Caches are synced for GC
I0119 07:53:14.369788       1 shared_informer.go:318] Caches are synced for TTL
I0119 07:53:14.370051       1 shared_informer.go:318] Caches are synced for HPA
I0119 07:53:14.370886       1 shared_informer.go:318] Caches are synced for service account
I0119 07:53:14.372554       1 shared_informer.go:318] Caches are synced for TTL after finished
I0119 07:53:14.374709       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0119 07:53:14.380548       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0119 07:53:14.380654       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0119 07:53:14.381803       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0119 07:53:14.385121       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0119 07:53:14.385361       1 shared_informer.go:318] Caches are synced for attach detach
I0119 07:53:14.386140       1 shared_informer.go:318] Caches are synced for job
I0119 07:53:14.386153       1 shared_informer.go:318] Caches are synced for PV protection
I0119 07:53:14.386295       1 shared_informer.go:318] Caches are synced for expand
I0119 07:53:14.390286       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0119 07:53:14.397561       1 shared_informer.go:318] Caches are synced for namespace
I0119 07:53:14.403626       1 shared_informer.go:318] Caches are synced for node
I0119 07:53:14.403812       1 range_allocator.go:174] "Sending events to api server"
I0119 07:53:14.404276       1 range_allocator.go:178] "Starting range CIDR allocator"
I0119 07:53:14.404345       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0119 07:53:14.404380       1 shared_informer.go:318] Caches are synced for cidrallocator
I0119 07:53:14.406317       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0119 07:53:14.410158       1 shared_informer.go:318] Caches are synced for PVC protection
I0119 07:53:14.415106       1 shared_informer.go:318] Caches are synced for taint
I0119 07:53:14.415572       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0119 07:53:14.417341       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0119 07:53:14.417535       1 taint_manager.go:211] "Sending events to api server"
I0119 07:53:14.417826       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0119 07:53:14.418829       1 shared_informer.go:318] Caches are synced for stateful set
I0119 07:53:14.419966       1 shared_informer.go:318] Caches are synced for daemon sets
I0119 07:53:14.420039       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0119 07:53:14.424349       1 shared_informer.go:318] Caches are synced for endpoint
I0119 07:53:14.424492       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0119 07:53:14.428764       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0119 07:53:14.450777       1 range_allocator.go:380] "Set node PodCIDR" node="minikube" podCIDRs=["10.244.0.0/24"]
I0119 07:53:14.473577       1 shared_informer.go:318] Caches are synced for deployment
I0119 07:53:14.487920       1 shared_informer.go:318] Caches are synced for disruption
I0119 07:53:14.488089       1 shared_informer.go:318] Caches are synced for resource quota
I0119 07:53:14.545876       1 shared_informer.go:318] Caches are synced for resource quota
I0119 07:53:14.632862       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-svl6k"
I0119 07:53:14.641820       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5dd5756b68 to 1"
I0119 07:53:14.768708       1 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5dd5756b68-q5kb4"
I0119 07:53:14.851322       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="211.426634ms"
I0119 07:53:14.880925       1 shared_informer.go:318] Caches are synced for garbage collector
I0119 07:53:14.886524       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="34.746247ms"
I0119 07:53:14.886997       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="301.968¬µs"
I0119 07:53:14.916631       1 shared_informer.go:318] Caches are synced for garbage collector
I0119 07:53:14.916792       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0119 07:53:16.761054       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="1.92339ms"
I0119 07:53:16.819466       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="353.682¬µs"
I0119 07:53:19.156544       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="5.227134ms"
I0119 07:53:20.215760       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="44.180734ms"
I0119 07:53:20.216238       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="249.082¬µs"

* 
* ==> kube-controller-manager [e432b83e9608] <==
* I0119 14:47:09.234389       1 node_lifecycle_controller.go:1029] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0119 14:47:09.235208       1 event.go:307] "Event occurred" object="kube-system/etcd-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 14:47:09.264039       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="87.714946ms"
I0119 14:47:09.264642       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="370.414¬µs"
I0119 14:47:15.515731       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="35.860699ms"
I0119 14:47:15.516565       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="220.499¬µs"
I0119 14:47:19.237326       1 node_lifecycle_controller.go:1048] "Controller detected that some Nodes are Ready. Exiting master disruption mode"
I0119 14:56:22.439948       1 event.go:307] "Event occurred" object="default/apim-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set apim-deployment-7b8dcdb9d8 to 1"
I0119 14:56:22.474541       1 event.go:307] "Event occurred" object="default/apim-deployment-7b8dcdb9d8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: apim-deployment-7b8dcdb9d8-xxfxp"
I0119 14:56:22.504834       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="65.252119ms"
I0119 14:56:22.539251       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="34.209157ms"
I0119 14:56:22.540194       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="530.013¬µs"
I0119 14:56:22.549531       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="283.07¬µs"
I0119 15:02:20.360934       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="34.56¬µs"
I0119 15:02:24.986147       1 event.go:307] "Event occurred" object="default/apim-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set apim-deployment-7b8dcdb9d8 to 1"
I0119 15:02:25.013078       1 event.go:307] "Event occurred" object="default/apim-deployment-7b8dcdb9d8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: apim-deployment-7b8dcdb9d8-jbw8l"
I0119 15:02:25.040653       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="57.62443ms"
I0119 15:02:25.078400       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="37.191401ms"
I0119 15:02:25.078813       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="240.401¬µs"
I0119 15:02:25.087832       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="248.209¬µs"
I0119 15:07:31.825758       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="30.963851ms"
I0119 15:07:31.828909       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="729.698¬µs"
I0119 15:27:45.098744       1 event.go:307] "Event occurred" object="default/apim-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set apim-deployment-f9c669cf6 to 1"
I0119 15:27:45.139162       1 event.go:307] "Event occurred" object="default/apim-deployment-f9c669cf6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: apim-deployment-f9c669cf6-d45cd"
I0119 15:27:45.184065       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-f9c669cf6" duration="104.417821ms"
I0119 15:27:45.205863       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-f9c669cf6" duration="21.561408ms"
I0119 15:27:45.207769       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-f9c669cf6" duration="288.469¬µs"
I0119 15:27:45.234375       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-f9c669cf6" duration="457.05¬µs"
I0119 15:27:45.312872       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-f9c669cf6" duration="1.150769ms"
I0119 15:27:48.300123       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-f9c669cf6" duration="28.766583ms"
I0119 15:27:48.301846       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-f9c669cf6" duration="165.303¬µs"
I0119 15:27:48.332023       1 event.go:307] "Event occurred" object="default/apim-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set apim-deployment-7b8dcdb9d8 to 0 from 1"
I0119 15:27:48.359037       1 event.go:307] "Event occurred" object="default/apim-deployment-7b8dcdb9d8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: apim-deployment-7b8dcdb9d8-jbw8l"
I0119 15:27:48.407926       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="81.73618ms"
I0119 15:27:48.428701       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="20.450871ms"
I0119 15:27:48.428967       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="135.962¬µs"
I0119 15:28:22.966399       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="10.970199ms"
I0119 15:28:23.520646       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="192.391¬µs"
I0119 15:28:23.600150       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="125.327¬µs"
I0119 16:07:02.710771       1 event.go:307] "Event occurred" object="default/apim" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set apim-7dcbfb86b4 to 1"
I0119 16:07:02.725005       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-f9c669cf6" duration="34.231¬µs"
I0119 16:07:02.725200       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7b8dcdb9d8" duration="38.661¬µs"
I0119 16:07:02.804007       1 event.go:307] "Event occurred" object="default/apim-7dcbfb86b4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: apim-7dcbfb86b4-klj65"
I0119 16:07:02.845507       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-7dcbfb86b4" duration="158.038811ms"
I0119 16:07:02.884116       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-7dcbfb86b4" duration="38.370454ms"
I0119 16:07:02.884513       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-7dcbfb86b4" duration="181.446¬µs"
I0119 16:07:02.902789       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-7dcbfb86b4" duration="157.219¬µs"
I0119 16:07:02.987965       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-7dcbfb86b4" duration="293.395¬µs"
I0119 16:07:05.421203       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-7dcbfb86b4" duration="21.187947ms"
I0119 16:07:05.421778       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-7dcbfb86b4" duration="327.04¬µs"
I0119 16:07:25.589972       1 event.go:307] "Event occurred" object="default/apim-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set apim-deployment-7dcbfb86b4 to 1"
I0119 16:07:25.642430       1 event.go:307] "Event occurred" object="default/apim-deployment-7dcbfb86b4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: apim-deployment-7dcbfb86b4-8zxvd"
I0119 16:07:25.714468       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-7dcbfb86b4" duration="135.868¬µs"
I0119 16:07:25.719855       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7dcbfb86b4" duration="151.423867ms"
I0119 16:07:25.839963       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7dcbfb86b4" duration="119.899738ms"
I0119 16:07:25.948549       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7dcbfb86b4" duration="107.594104ms"
I0119 16:07:25.950163       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7dcbfb86b4" duration="1.282469ms"
I0119 16:07:25.977420       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7dcbfb86b4" duration="253.555¬µs"
I0119 16:07:30.674431       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7dcbfb86b4" duration="86.647231ms"
I0119 16:07:30.674957       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/apim-deployment-7dcbfb86b4" duration="234.854¬µs"

* 
* ==> kube-proxy [2ba7dca3e8aa] <==
* I0119 07:53:16.897424       1 server_others.go:69] "Using iptables proxy"
I0119 07:53:16.951624       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0119 07:53:17.063225       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0119 07:53:17.070454       1 server_others.go:152] "Using iptables Proxier"
I0119 07:53:17.070587       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0119 07:53:17.070613       1 server_others.go:438] "Defaulting to no-op detect-local"
I0119 07:53:17.071970       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0119 07:53:17.073119       1 server.go:846] "Version info" version="v1.28.3"
I0119 07:53:17.073224       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0119 07:53:17.085473       1 config.go:188] "Starting service config controller"
I0119 07:53:17.085549       1 config.go:97] "Starting endpoint slice config controller"
I0119 07:53:17.085636       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0119 07:53:17.085566       1 shared_informer.go:311] Waiting for caches to sync for service config
I0119 07:53:17.087062       1 config.go:315] "Starting node config controller"
I0119 07:53:17.087238       1 shared_informer.go:311] Waiting for caches to sync for node config
I0119 07:53:17.186083       1 shared_informer.go:318] Caches are synced for service config
I0119 07:53:17.186122       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0119 07:53:17.187425       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [2fc8f7ff5b55] <==
* I0119 08:10:50.590126       1 server_others.go:69] "Using iptables proxy"
I0119 08:10:50.724929       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0119 08:10:51.061062       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0119 08:10:51.085517       1 server_others.go:152] "Using iptables Proxier"
I0119 08:10:51.085694       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0119 08:10:51.085735       1 server_others.go:438] "Defaulting to no-op detect-local"
I0119 08:10:51.091641       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0119 08:10:51.092880       1 server.go:846] "Version info" version="v1.28.3"
I0119 08:10:51.092963       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0119 08:10:51.116857       1 config.go:97] "Starting endpoint slice config controller"
I0119 08:10:51.141727       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0119 08:10:51.120830       1 config.go:188] "Starting service config controller"
I0119 08:10:51.143316       1 shared_informer.go:311] Waiting for caches to sync for service config
I0119 08:10:51.151327       1 config.go:315] "Starting node config controller"
I0119 08:10:51.151479       1 shared_informer.go:311] Waiting for caches to sync for node config
I0119 08:10:51.245559       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0119 08:10:51.245680       1 shared_informer.go:318] Caches are synced for service config
I0119 08:10:51.252976       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [29ef249ddfa5] <==
* E0119 07:52:57.037118       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0119 07:52:57.037987       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0119 07:52:57.038155       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0119 07:52:57.038934       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0119 07:52:57.039081       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0119 07:52:57.038940       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0119 07:52:57.039495       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0119 07:52:57.039549       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0119 07:52:57.039532       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0119 07:52:57.039662       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0119 07:52:57.039721       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0119 07:52:57.040716       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0119 07:52:57.041077       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0119 07:52:57.044492       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0119 07:52:57.044517       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0119 07:52:57.044893       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0119 07:52:57.041447       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0119 07:52:57.045093       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0119 07:52:57.045121       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0119 07:52:57.045274       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0119 07:52:57.045698       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0119 07:52:57.045799       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0119 07:52:57.045857       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0119 07:52:57.045924       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0119 07:52:57.046330       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0119 07:52:57.046397       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0119 07:52:58.167619       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0119 07:52:58.167794       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0119 07:52:58.280291       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0119 07:52:58.280561       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0119 07:52:58.337695       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0119 07:52:58.337834       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0119 07:52:58.409688       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0119 07:52:58.409849       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0119 07:52:58.442910       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0119 07:52:58.443006       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0119 07:52:58.454112       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0119 07:52:58.454272       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0119 07:52:58.470952       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0119 07:52:58.471054       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0119 07:52:58.471713       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0119 07:52:58.471809       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0119 07:52:58.483808       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0119 07:52:58.483929       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0119 07:52:58.528452       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0119 07:52:58.528692       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0119 07:52:58.530543       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0119 07:52:58.530649       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0119 07:52:58.553505       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0119 07:52:58.553628       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0119 07:52:58.570625       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0119 07:52:58.570759       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0119 07:52:58.623282       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0119 07:52:58.623438       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0119 07:52:58.641523       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0119 07:52:58.641659       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
I0119 07:53:01.106600       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0119 08:02:25.145980       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0119 08:02:25.151774       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E0119 08:02:25.155940       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [2b3e02024c60] <==
* I0119 08:10:40.482745       1 serving.go:348] Generated self-signed cert in-memory
W0119 08:10:47.626663       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0119 08:10:47.627342       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0119 08:10:47.627790       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0119 08:10:47.628160       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0119 08:10:47.762177       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0119 08:10:47.771283       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0119 08:10:47.784587       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0119 08:10:47.789810       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0119 08:10:47.793664       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0119 08:10:47.820754       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0119 08:10:47.894563       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Jan 19 14:56:22 minikube kubelet[1480]: I0119 14:56:22.657012    1480 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-h6t4d\" (UniqueName: \"kubernetes.io/projected/0558a6d2-7bcf-495e-b98e-12a07370d371-kube-api-access-h6t4d\") pod \"apim-deployment-7b8dcdb9d8-xxfxp\" (UID: \"0558a6d2-7bcf-495e-b98e-12a07370d371\") " pod="default/apim-deployment-7b8dcdb9d8-xxfxp"
Jan 19 15:00:34 minikube kubelet[1480]: W0119 15:00:34.995943    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 15:02:25 minikube kubelet[1480]: I0119 15:02:25.030886    1480 topology_manager.go:215] "Topology Admit Handler" podUID="36d35a28-a956-45ec-b244-2732c5f8a5ab" podNamespace="default" podName="apim-deployment-7b8dcdb9d8-jbw8l"
Jan 19 15:02:25 minikube kubelet[1480]: I0119 15:02:25.077929    1480 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hr85h\" (UniqueName: \"kubernetes.io/projected/36d35a28-a956-45ec-b244-2732c5f8a5ab-kube-api-access-hr85h\") pod \"apim-deployment-7b8dcdb9d8-jbw8l\" (UID: \"36d35a28-a956-45ec-b244-2732c5f8a5ab\") " pod="default/apim-deployment-7b8dcdb9d8-jbw8l"
Jan 19 15:02:25 minikube kubelet[1480]: I0119 15:02:25.954783    1480 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d6bb62e6840d8775bb47b94da0cafcf25bc39ac5e0c99c457c3cc1a0e2649579"
Jan 19 15:05:34 minikube kubelet[1480]: W0119 15:05:34.998915    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 15:07:31 minikube kubelet[1480]: I0119 15:07:31.799383    1480 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/apim-deployment-7b8dcdb9d8-xxfxp" podStartSLOduration=9.091365767 podCreationTimestamp="2024-01-19 14:56:22 +0000 UTC" firstStartedPulling="2024-01-19 14:56:23.625935187 +0000 UTC m=+24350.499853109" lastFinishedPulling="2024-01-19 15:07:24.34485581 +0000 UTC m=+25011.204018220" observedRunningTime="2024-01-19 15:07:25.471907842 +0000 UTC m=+25012.331071227" watchObservedRunningTime="2024-01-19 15:07:31.795530878 +0000 UTC m=+25018.654692778"
Jan 19 15:07:31 minikube kubelet[1480]: I0119 15:07:31.802361    1480 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/apim-deployment-7b8dcdb9d8-jbw8l" podStartSLOduration=2.83073949 podCreationTimestamp="2024-01-19 15:02:24 +0000 UTC" firstStartedPulling="2024-01-19 15:02:26.108234272 +0000 UTC m=+24712.970180798" lastFinishedPulling="2024-01-19 15:07:31.082486764 +0000 UTC m=+25017.941648617" observedRunningTime="2024-01-19 15:07:31.801984291 +0000 UTC m=+25018.661146459" watchObservedRunningTime="2024-01-19 15:07:31.802207309 +0000 UTC m=+25018.661370412"
Jan 19 15:07:57 minikube kubelet[1480]: I0119 15:07:57.059934    1480 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-h6t4d\" (UniqueName: \"kubernetes.io/projected/0558a6d2-7bcf-495e-b98e-12a07370d371-kube-api-access-h6t4d\") pod \"0558a6d2-7bcf-495e-b98e-12a07370d371\" (UID: \"0558a6d2-7bcf-495e-b98e-12a07370d371\") "
Jan 19 15:07:57 minikube kubelet[1480]: I0119 15:07:57.182248    1480 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/0558a6d2-7bcf-495e-b98e-12a07370d371-kube-api-access-h6t4d" (OuterVolumeSpecName: "kube-api-access-h6t4d") pod "0558a6d2-7bcf-495e-b98e-12a07370d371" (UID: "0558a6d2-7bcf-495e-b98e-12a07370d371"). InnerVolumeSpecName "kube-api-access-h6t4d". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jan 19 15:07:57 minikube kubelet[1480]: I0119 15:07:57.281965    1480 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-h6t4d\" (UniqueName: \"kubernetes.io/projected/0558a6d2-7bcf-495e-b98e-12a07370d371-kube-api-access-h6t4d\") on node \"minikube\" DevicePath \"\""
Jan 19 15:07:57 minikube kubelet[1480]: I0119 15:07:57.735831    1480 scope.go:117] "RemoveContainer" containerID="148da073e3b42537bf6c2759a8fc83fba5289937383d8ac402384179a8f83059"
Jan 19 15:07:59 minikube kubelet[1480]: I0119 15:07:59.111673    1480 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="0558a6d2-7bcf-495e-b98e-12a07370d371" path="/var/lib/kubelet/pods/0558a6d2-7bcf-495e-b98e-12a07370d371/volumes"
Jan 19 15:10:35 minikube kubelet[1480]: W0119 15:10:35.022813    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 15:15:35 minikube kubelet[1480]: W0119 15:15:35.012791    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 15:20:34 minikube kubelet[1480]: W0119 15:20:34.834382    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 15:25:34 minikube kubelet[1480]: W0119 15:25:34.787440    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 15:27:45 minikube kubelet[1480]: I0119 15:27:45.251652    1480 topology_manager.go:215] "Topology Admit Handler" podUID="ab534a96-e2d7-4242-99ba-c64b8837411b" podNamespace="default" podName="apim-deployment-f9c669cf6-d45cd"
Jan 19 15:27:45 minikube kubelet[1480]: E0119 15:27:45.253721    1480 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="0558a6d2-7bcf-495e-b98e-12a07370d371" containerName="apim-deployment"
Jan 19 15:27:45 minikube kubelet[1480]: I0119 15:27:45.257462    1480 memory_manager.go:346] "RemoveStaleState removing state" podUID="0558a6d2-7bcf-495e-b98e-12a07370d371" containerName="apim-deployment"
Jan 19 15:27:45 minikube kubelet[1480]: I0119 15:27:45.410432    1480 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-s9wzt\" (UniqueName: \"kubernetes.io/projected/ab534a96-e2d7-4242-99ba-c64b8837411b-kube-api-access-s9wzt\") pod \"apim-deployment-f9c669cf6-d45cd\" (UID: \"ab534a96-e2d7-4242-99ba-c64b8837411b\") " pod="default/apim-deployment-f9c669cf6-d45cd"
Jan 19 15:27:47 minikube kubelet[1480]: I0119 15:27:47.105212    1480 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="8fdfb4990c2c11e38a496457baba3ec92be2e11b06c75ced28ef28e70cafaae1"
Jan 19 15:27:48 minikube kubelet[1480]: I0119 15:27:48.364937    1480 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/apim-deployment-f9c669cf6-d45cd" podStartSLOduration=3.356152583 podCreationTimestamp="2024-01-19 15:27:45 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-19 15:27:48.270208634 +0000 UTC m=+26235.345548094" watchObservedRunningTime="2024-01-19 15:27:48.356152583 +0000 UTC m=+26235.431490639"
Jan 19 15:28:22 minikube kubelet[1480]: I0119 15:28:22.772635    1480 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-hr85h\" (UniqueName: \"kubernetes.io/projected/36d35a28-a956-45ec-b244-2732c5f8a5ab-kube-api-access-hr85h\") pod \"36d35a28-a956-45ec-b244-2732c5f8a5ab\" (UID: \"36d35a28-a956-45ec-b244-2732c5f8a5ab\") "
Jan 19 15:28:22 minikube kubelet[1480]: I0119 15:28:22.812809    1480 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/36d35a28-a956-45ec-b244-2732c5f8a5ab-kube-api-access-hr85h" (OuterVolumeSpecName: "kube-api-access-hr85h") pod "36d35a28-a956-45ec-b244-2732c5f8a5ab" (UID: "36d35a28-a956-45ec-b244-2732c5f8a5ab"). InnerVolumeSpecName "kube-api-access-hr85h". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jan 19 15:28:22 minikube kubelet[1480]: I0119 15:28:22.876292    1480 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-hr85h\" (UniqueName: \"kubernetes.io/projected/36d35a28-a956-45ec-b244-2732c5f8a5ab-kube-api-access-hr85h\") on node \"minikube\" DevicePath \"\""
Jan 19 15:28:23 minikube kubelet[1480]: I0119 15:28:23.364185    1480 scope.go:117] "RemoveContainer" containerID="6a573a92d44c198fb7a5cdec2a38c4cb5b2b186be1c45990517262a3123b3ed1"
Jan 19 15:28:24 minikube kubelet[1480]: I0119 15:28:24.865186    1480 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="36d35a28-a956-45ec-b244-2732c5f8a5ab" path="/var/lib/kubelet/pods/36d35a28-a956-45ec-b244-2732c5f8a5ab/volumes"
Jan 19 15:30:34 minikube kubelet[1480]: W0119 15:30:34.788154    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 15:35:34 minikube kubelet[1480]: W0119 15:35:34.785038    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 15:40:34 minikube kubelet[1480]: W0119 15:40:34.761587    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 15:45:34 minikube kubelet[1480]: W0119 15:45:34.751618    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 15:50:34 minikube kubelet[1480]: W0119 15:50:34.731049    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 15:55:34 minikube kubelet[1480]: W0119 15:55:34.716523    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 16:00:34 minikube kubelet[1480]: W0119 16:00:34.777601    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 16:05:34 minikube kubelet[1480]: W0119 16:05:34.776499    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 19 16:07:02 minikube kubelet[1480]: I0119 16:07:02.907134    1480 topology_manager.go:215] "Topology Admit Handler" podUID="49bb3947-73e3-4de6-a59a-dcf41b9fde90" podNamespace="default" podName="apim-7dcbfb86b4-klj65"
Jan 19 16:07:02 minikube kubelet[1480]: E0119 16:07:02.913647    1480 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="36d35a28-a956-45ec-b244-2732c5f8a5ab" containerName="apim-deployment"
Jan 19 16:07:02 minikube kubelet[1480]: I0119 16:07:02.919802    1480 memory_manager.go:346] "RemoveStaleState removing state" podUID="36d35a28-a956-45ec-b244-2732c5f8a5ab" containerName="apim-deployment"
Jan 19 16:07:02 minikube kubelet[1480]: I0119 16:07:02.973198    1480 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-wb86n\" (UniqueName: \"kubernetes.io/projected/49bb3947-73e3-4de6-a59a-dcf41b9fde90-kube-api-access-wb86n\") pod \"apim-7dcbfb86b4-klj65\" (UID: \"49bb3947-73e3-4de6-a59a-dcf41b9fde90\") " pod="default/apim-7dcbfb86b4-klj65"
Jan 19 16:07:04 minikube kubelet[1480]: I0119 16:07:04.272204    1480 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3f28ecb3bb8b58a9782efc06b5570ff2ab1adb55a229c37369fc575e5350ff6d"
Jan 19 16:07:25 minikube kubelet[1480]: I0119 16:07:25.837377    1480 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/apim-7dcbfb86b4-klj65" podStartSLOduration=23.809496036 podCreationTimestamp="2024-01-19 16:07:02 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-19 16:07:05.399142159 +0000 UTC m=+28592.483068037" watchObservedRunningTime="2024-01-19 16:07:25.809496036 +0000 UTC m=+28612.893834350"
Jan 19 16:07:25 minikube kubelet[1480]: I0119 16:07:25.855784    1480 topology_manager.go:215] "Topology Admit Handler" podUID="168ccbf5-fde1-4047-a99f-218beebffa79" podNamespace="default" podName="apim-deployment-7dcbfb86b4-8zxvd"
Jan 19 16:07:25 minikube kubelet[1480]: I0119 16:07:25.983519    1480 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-nb6tb\" (UniqueName: \"kubernetes.io/projected/168ccbf5-fde1-4047-a99f-218beebffa79-kube-api-access-nb6tb\") pod \"apim-deployment-7dcbfb86b4-8zxvd\" (UID: \"168ccbf5-fde1-4047-a99f-218beebffa79\") " pod="default/apim-deployment-7dcbfb86b4-8zxvd"
Jan 19 16:07:27 minikube kubelet[1480]: I0119 16:07:27.776677    1480 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e62042f58c2f7952874196ca02072f8920c63c4b09481bcec7b50141cf5f3772"
Jan 19 16:07:30 minikube kubelet[1480]: I0119 16:07:30.583555    1480 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/apim-deployment-7dcbfb86b4-8zxvd" podStartSLOduration=5.583405664 podCreationTimestamp="2024-01-19 16:07:25 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-19 16:07:30.580514087 +0000 UTC m=+28617.664853029" watchObservedRunningTime="2024-01-19 16:07:30.583405664 +0000 UTC m=+28617.667744005"
Jan 19 16:07:36 minikube kubelet[1480]: I0119 16:07:36.105216    1480 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-s9wzt\" (UniqueName: \"kubernetes.io/projected/ab534a96-e2d7-4242-99ba-c64b8837411b-kube-api-access-s9wzt\") pod \"ab534a96-e2d7-4242-99ba-c64b8837411b\" (UID: \"ab534a96-e2d7-4242-99ba-c64b8837411b\") "
Jan 19 16:07:36 minikube kubelet[1480]: I0119 16:07:36.196751    1480 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/ab534a96-e2d7-4242-99ba-c64b8837411b-kube-api-access-s9wzt" (OuterVolumeSpecName: "kube-api-access-s9wzt") pod "ab534a96-e2d7-4242-99ba-c64b8837411b" (UID: "ab534a96-e2d7-4242-99ba-c64b8837411b"). InnerVolumeSpecName "kube-api-access-s9wzt". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jan 19 16:07:36 minikube kubelet[1480]: I0119 16:07:36.208943    1480 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-s9wzt\" (UniqueName: \"kubernetes.io/projected/ab534a96-e2d7-4242-99ba-c64b8837411b-kube-api-access-s9wzt\") on node \"minikube\" DevicePath \"\""
Jan 19 16:07:36 minikube kubelet[1480]: I0119 16:07:36.399791    1480 scope.go:117] "RemoveContainer" containerID="0088bdedd4a732b33579445acd0626557f09873d67127cd308e4bd6333ef4c97"
Jan 19 16:07:36 minikube kubelet[1480]: I0119 16:07:36.919931    1480 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="ab534a96-e2d7-4242-99ba-c64b8837411b" path="/var/lib/kubelet/pods/ab534a96-e2d7-4242-99ba-c64b8837411b/volumes"
Jan 19 16:07:56 minikube kubelet[1480]: I0119 16:07:56.967880    1480 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-wb86n\" (UniqueName: \"kubernetes.io/projected/49bb3947-73e3-4de6-a59a-dcf41b9fde90-kube-api-access-wb86n\") pod \"49bb3947-73e3-4de6-a59a-dcf41b9fde90\" (UID: \"49bb3947-73e3-4de6-a59a-dcf41b9fde90\") "
Jan 19 16:07:56 minikube kubelet[1480]: I0119 16:07:56.976373    1480 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/49bb3947-73e3-4de6-a59a-dcf41b9fde90-kube-api-access-wb86n" (OuterVolumeSpecName: "kube-api-access-wb86n") pod "49bb3947-73e3-4de6-a59a-dcf41b9fde90" (UID: "49bb3947-73e3-4de6-a59a-dcf41b9fde90"). InnerVolumeSpecName "kube-api-access-wb86n". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jan 19 16:07:57 minikube kubelet[1480]: I0119 16:07:57.068527    1480 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-wb86n\" (UniqueName: \"kubernetes.io/projected/49bb3947-73e3-4de6-a59a-dcf41b9fde90-kube-api-access-wb86n\") on node \"minikube\" DevicePath \"\""
Jan 19 16:07:57 minikube kubelet[1480]: I0119 16:07:57.388707    1480 scope.go:117] "RemoveContainer" containerID="a90edb494cb8b1615db30a3b9843f7a9879b2a0971f7909708c93834203853f5"
Jan 19 16:07:57 minikube kubelet[1480]: I0119 16:07:57.512638    1480 scope.go:117] "RemoveContainer" containerID="a90edb494cb8b1615db30a3b9843f7a9879b2a0971f7909708c93834203853f5"
Jan 19 16:07:57 minikube kubelet[1480]: E0119 16:07:57.527765    1480 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: a90edb494cb8b1615db30a3b9843f7a9879b2a0971f7909708c93834203853f5" containerID="a90edb494cb8b1615db30a3b9843f7a9879b2a0971f7909708c93834203853f5"
Jan 19 16:07:57 minikube kubelet[1480]: I0119 16:07:57.528262    1480 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"a90edb494cb8b1615db30a3b9843f7a9879b2a0971f7909708c93834203853f5"} err="failed to get container status \"a90edb494cb8b1615db30a3b9843f7a9879b2a0971f7909708c93834203853f5\": rpc error: code = Unknown desc = Error response from daemon: No such container: a90edb494cb8b1615db30a3b9843f7a9879b2a0971f7909708c93834203853f5"
Jan 19 16:07:58 minikube kubelet[1480]: I0119 16:07:58.864277    1480 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="49bb3947-73e3-4de6-a59a-dcf41b9fde90" path="/var/lib/kubelet/pods/49bb3947-73e3-4de6-a59a-dcf41b9fde90/volumes"
Jan 19 16:10:34 minikube kubelet[1480]: W0119 16:10:34.766149    1480 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> storage-provisioner [6866be2b7491] <==
* I0119 08:11:35.331041       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0119 08:11:35.387934       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0119 08:11:35.388653       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0119 08:11:52.820517       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0119 08:11:52.820893       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"d664676e-52d5-4237-9bd5-082b22e12629", APIVersion:"v1", ResourceVersion:"965", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_b0a79407-f2af-47b2-a4be-95768bfe5e36 became leader
I0119 08:11:52.822948       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_b0a79407-f2af-47b2-a4be-95768bfe5e36!
I0119 08:11:52.923907       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_b0a79407-f2af-47b2-a4be-95768bfe5e36!

* 
* ==> storage-provisioner [8662f984cc4f] <==
* I0119 08:10:50.211049       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0119 08:11:20.314915       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

